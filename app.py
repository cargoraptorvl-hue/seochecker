#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEO Audit Machine - Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ SEO-–∞—É–¥–∏—Ç–∞.

pip install streamlit requests beautifulsoup4 lxml pandas plotly fake-useragent openpyxl urllib3
"""

# === IMPORTS ===
import hashlib
import io
import json
import logging
import os
import re
import socket
import ssl
import threading
import time
import uuid
from collections import Counter, defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime
from difflib import SequenceMatcher
from typing import Any, Dict, List, Optional, Set, Tuple
from urllib.parse import (
    parse_qs, urlencode, urljoin, urlparse, urlunparse, quote
)
from urllib.robotparser import RobotFileParser

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import requests
import streamlit as st
import urllib3
from bs4 import BeautifulSoup, Comment
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

try:
    from fake_useragent import UserAgent
    _UA = UserAgent(fallback="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
except Exception:
    _UA = None

# === LOGGING ===
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("seo_audit")

# === CONSTANTS ===

PENALTY_WEIGHTS = {"critical": 5, "warning": 2, "info": 0}
MAX_CATEGORY_PENALTY = 30

SEVERITY_ORDER = {"critical": 0, "warning": 1, "info": 2}

# Thresholds
# Title thresholds: Google shows ~55-60 chars, Yandex shows ~65-70.
# Using 20/70 to cover both engines and reduce false positives.
TITLE_MIN = 20
TITLE_MAX = 70
DESC_MIN = 70
DESC_MAX = 160
H1_MAX_LEN = 70
THIN_CONTENT_THRESHOLD = 300
VERY_THIN_CONTENT_THRESHOLD = 100
TEXT_HTML_RATIO_LOW = 10
TTFB_WARNING = 1.5
TTFB_CRITICAL = 3.0
MAX_REDIRECTS = 5
LARGE_IMAGE_BYTES = 500 * 1024  # 500KB

IS_RENDER = os.getenv("RENDER", "").lower() == "true" or bool(os.getenv("RENDER_SERVICE_ID"))
MAX_WORKERS = 3 if IS_RENDER else 5
MAX_HTML_SIZE = 2 * 1024 * 1024 if IS_RENDER else 5 * 1024 * 1024  # bytes
MAX_STORED_HTML_CHARS = 120000
MAX_CONTENT_TEXT_CHARS = 12000
MAX_EXTERNAL_CHECKS = 200 if IS_RENDER else 400
MAX_UNCRAWLED_STATUS_CHECKS = 250 if IS_RENDER else 500
UI_MAX_PAGES_LIMIT = 120 if IS_RENDER else 200
UI_DEFAULT_MAX_PAGES = 40 if IS_RENDER else 50
MAX_IMAGE_RESOURCE_CHECKS = 3 if IS_RENDER else 8
RESOURCE_CHECK_TIMEOUT = 4 if IS_RENDER else 6
# Global cap on total image/resource HEAD checks across all pages
MAX_TOTAL_RESOURCE_CHECKS = 60 if IS_RENDER else 200
MAX_SIMILARITY_PAIRS = 2500 if IS_RENDER else 7000

# –ü—Ä–æ—Å—Ç–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤ Render -> Environment)
AUTH_USERNAME = os.getenv("SEO_AUDIT_USERNAME", "1")
AUTH_PASSWORD = os.getenv("SEO_AUDIT_PASSWORD", "123")
AUTH_TTL_SECONDS = int(os.getenv("SEO_AUDIT_AUTH_TTL_SECONDS", "1800"))  # 30 –º–∏–Ω—É—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

# URL patterns that often cause infinite crawl traps
TRAP_PATTERNS = [
    r'/\?.*page=\d+',
    r'/\?.*sort=',
    r'/\?.*filter=',
    r'/calendar/',
    r'/tag/',
    r'/\d{4}/\d{2}/\d{2}/',
]

RECOMMENDATIONS_RU = {
    "broken_links": {
        "title": "–ò—Å–ø—Ä–∞–≤–∏—Ç—å {count} –±–∏—Ç—ã—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ (404)",
        "impact": "–ü–æ–∏—Å–∫–æ–≤—ã–µ —Ä–æ–±–æ—Ç—ã –Ω–µ –º–æ–≥—É—Ç –ø—Ä–æ–π—Ç–∏ –ø–æ —ç—Ç–∏–º —Å—Å—ã–ª–∫–∞–º -- —Ç—Ä–∞—Ç–∏—Ç—Å—è –∫—Ä–∞—É–ª–∏–Ω–≥–æ–≤—ã–π –±—é–¥–∂–µ—Ç –≤–ø—É—Å—Ç—É—é, –∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤–∏–¥—è—Ç –æ—à–∏–±–∫—É.",
        "fix": "–ù–∞—Å—Ç—Ä–æ–∏—Ç—å 301-—Ä–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å/—É–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏ –≤ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–µ.",
        "effort": "quick",
    },
    "missing_h1": {
        "title": "–î–æ–±–∞–≤–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ H1 –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü",
        "impact": "–ë–µ–∑ H1 –ø–æ–∏—Å–∫–æ–≤–∏–∫ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –≠—Ç–æ –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ.",
        "fix": "–í —Ä–µ–¥–∞–∫—Ç–æ—Ä–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–±–∞–≤–∏—Ç—å –æ–¥–∏–Ω —Ç–µ–≥ H1 —Å –æ—Å–Ω–æ–≤–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º.",
        "effort": "quick",
    },
    "multiple_h1": {
        "title": "–£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ H1 –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö (–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ –æ–¥–Ω–æ–º—É)",
        "impact": "–ù–µ—Å–∫–æ–ª—å–∫–æ H1 —Ä–∞–∑–º—ã–≤–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å. –ü–æ–∏—Å–∫–æ–≤–∏–∫ –Ω–µ –∑–Ω–∞–µ—Ç, –∫–∞–∫–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥–ª–∞–≤–Ω—ã–π.",
        "fix": "–û—Å—Ç–∞–≤–∏—Ç—å –æ–¥–∏–Ω H1, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–Ω–∏–∑–∏—Ç—å –¥–æ H2 –∏–ª–∏ H3.",
        "effort": "quick",
    },
    "missing_title": {
        "title": "–î–æ–±–∞–≤–∏—Ç—å Title –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü",
        "impact": "–ë–µ–∑ Title —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–≤–∏–¥–∏–º–∞ –≤ –ø–æ–∏—Å–∫–µ -- —Å–Ω–∏–ø–ø–µ—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω–æ.",
        "fix": "–ó–∞–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–µ Title –≤ SEO-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (30-60 —Å–∏–º–≤–æ–ª–æ–≤, —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º).",
        "effort": "medium",
    },
    "empty_title": {
        "title": "–ó–∞–ø–æ–ª–Ω–∏—Ç—å –ø—É—Å—Ç—ã–µ Title –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö",
        "impact": "–ü—É—Å—Ç–æ–π Title —Ä–∞–≤–Ω–æ–∑–Ω–∞—á–µ–Ω –µ–≥–æ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—é -- Google —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, —á–∞—Å—Ç–æ –Ω–µ—É–¥–∞—á–Ω–æ.",
        "fix": "–ù–∞–ø–∏—Å–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π Title 30-60 —Å–∏–º–≤–æ–ª–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.",
        "effort": "medium",
    },
    "missing_description": {
        "title": "–ù–∞–ø–∏—Å–∞—Ç—å Meta Description –¥–ª—è {count} —Å—Ç—Ä–∞–Ω–∏—Ü",
        "impact": "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è Google/–Ø–Ω–¥–µ–∫—Å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã -- —Å–Ω–∏–∂–∞–µ—Ç—Å—è CTR.",
        "fix": "–ù–∞–ø–∏—Å–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ 150-160 —Å–∏–º–≤–æ–ª–æ–≤ —Å –£–¢–ü –∏ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞.",
        "effort": "medium",
    },
    "duplicate_titles": {
        "title": "–†–∞–∑–Ω–µ—Å—Ç–∏ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è Title ({count} –≥—Ä—É–ø–ø)",
        "impact": "–û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å–æ–∑–¥–∞—é—Ç –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–∞—Ü–∏—é -- —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—Ç –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º –≤ –ø–æ–∏—Å–∫–µ.",
        "fix": "–°–¥–µ–ª–∞—Ç—å –∫–∞–∂–¥—ã–π Title —É–Ω–∏–∫–∞–ª—å–Ω—ã–º, –¥–æ–±–∞–≤–∏–≤ —Å–ø–µ—Ü–∏—Ñ–∏–∫—É: –≥–æ—Ä–æ–¥, —Ç–∏–ø —É—Å–ª—É–≥–∏, —Ü–µ–Ω—É.",
        "effort": "medium",
    },
    "duplicate_descriptions": {
        "title": "–°–¥–µ–ª–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ Description ({count} –≥—Ä—É–ø–ø)",
        "impact": "–î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –æ–ø–∏—Å–∞–Ω–∏—è -- —Å–∏–≥–Ω–∞–ª –¥–ª—è Google/–Ø–Ω–¥–µ–∫—Å –æ –Ω–∏–∑–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ —Å—Ç—Ä–∞–Ω–∏—Ü.",
        "fix": "–ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.",
        "effort": "medium",
    },
    "missing_alt": {
        "title": "–ü—Ä–æ–ø–∏—Å–∞—Ç—å ALT-–∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        "impact": "–ë–µ–∑ ALT –∫–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –ø–æ–∏—Å–∫ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏ —É—Ö—É–¥—à–∞–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å.",
        "fix": "–î–æ–±–∞–≤–∏—Ç—å –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤ –ø–æ–ª–µ ALT –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.",
        "effort": "medium",
    },
    "missing_canonical": {
        "title": "–ù–∞—Å—Ç—Ä–æ–∏—Ç—å Canonical –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö",
        "impact": "–ë–µ–∑ canonical –ø–æ–∏—Å–∫–æ–≤–∏–∫ –º–æ–∂–µ—Ç –≤—ã–±—Ä–∞—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é.",
        "fix": "–í SEO-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö —É–∫–∞–∑–∞—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL (https://...) –∫–∞–∫ canonical –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.",
        "effort": "medium",
    },
    "slow_pages": {
        "title": "–£—Å–∫–æ—Ä–∏—Ç—å {count} –º–µ–¥–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (TTFB > 2 —Å–µ–∫)",
        "impact": "–ú–µ–¥–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ = –ø–æ—Ç–µ—Ä—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π (Core Web Vitals).",
        "fix": "–°–∂–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∫–ª—é—á–∏—Ç—å Lazy Load, —É–±—Ä–∞—Ç—å —Ç—è–∂—ë–ª—ã–µ –∞–Ω–∏–º–∞—Ü–∏–∏ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö.",
        "effort": "complex",
    },
    "no_schema": {
        "title": "–í–Ω–µ–¥—Ä–∏—Ç—å –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É Schema.org",
        "impact": "–ë–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–Ω–∏–ø–ø–µ—Ç—ã (–∑–≤—ë–∑–¥—ã, FAQ, —Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏).",
        "fix": "–î–æ–±–∞–≤–∏—Ç—å JSON-LD —Ä–∞–∑–º–µ—Ç–∫—É: Organization, LocalBusiness, FAQPage, BreadcrumbList.",
        "effort": "complex",
    },
    "no_https_redirect": {
        "title": "–ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç HTTP -> HTTPS",
        "impact": "–ë–µ–∑ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞ —á–∞—Å—Ç—å —Ç—Ä–∞—Ñ–∏–∫–∞ –∏–¥—ë—Ç –Ω–∞ –Ω–µ–∑–∞—â–∏—â—ë–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é -- Google –º–æ–∂–µ—Ç –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –æ–±–µ.",
        "fix": "–í –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö —Ö–æ—Å—Ç–∏–Ω–≥–∞/–¥–æ–º–µ–Ω–∞ –≤–∫–ª—é—á–∏—Ç—å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π HTTPS-—Ä–µ–¥–∏—Ä–µ–∫—Ç (301).",
        "effort": "quick",
    },
    "redirect_chains": {
        "title": "–£—Å—Ç—Ä–∞–Ω–∏—Ç—å {count} —Ü–µ–ø–æ—á–µ–∫ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤",
        "impact": "–ö–∞–∂–¥—ã–π –ª–∏—à–Ω–∏–π —Ö–æ–ø —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Ç—Ä–∞—Ç–∏—Ç –∫—Ä–∞—É–ª–∏–Ω–≥–æ–≤—ã–π –±—é–¥–∂–µ—Ç.",
        "fix": "–ó–∞–º–µ–Ω–∏—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –ø—Ä—è–º—ã–º 301 –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–π URL.",
        "effort": "medium",
    },
    "missing_viewport": {
        "title": "–î–æ–±–∞–≤–∏—Ç—å meta viewport –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü",
        "impact": "–ë–µ–∑ viewport –º–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ -- Google —Å–Ω–∏–∂–∞–µ—Ç –º–æ–±–∏–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥.",
        "fix": '–î–æ–±–∞–≤–∏—Ç—å <meta name="viewport" content="width=device-width, initial-scale=1"> –≤ head.',
        "effort": "quick",
    },
    "orphan_pages": {
        "title": "–î–æ–±–∞–≤–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ {count} —Å–∏—Ä–æ—Ç—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü",
        "impact": "–°—Ç—Ä–∞–Ω–∏—Ü—ã –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ –ø–ª–æ—Ö–æ –∏–Ω–¥–µ–∫—Å–∏—Ä—É—é—Ç—Å—è -- —Ä–æ–±–æ—Ç—ã –∏—Ö –Ω–µ –Ω–∞—Ö–æ–¥—è—Ç.",
        "fix": "–î–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ —ç—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, –±–ª–æ–≥–∞ –∏–ª–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤.",
        "effort": "medium",
    },
    "thin_content": {
        "title": "–†–∞—Å—à–∏—Ä–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö (<300 —Å–ª–æ–≤)",
        "impact": "–¢–æ–Ω–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è –∫–∞–∫ –º–∞–ª–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π -- Google/–Ø–Ω–¥–µ–∫—Å –Ω–µ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ç–∞–∫–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.",
        "fix": "–î–æ–ø–æ–ª–Ω–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–ª–µ–∑–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º: FAQ, –æ–ø–∏—Å–∞–Ω–∏—è —É—Å–ª—É–≥, –∫–µ–π—Å—ã, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.",
        "effort": "complex",
    },
    "missing_og": {
        "title": "–î–æ–±–∞–≤–∏—Ç—å Open Graph —Ç–µ–≥–∏ –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü",
        "impact": "–ë–µ–∑ OG-—Ç–µ–≥–æ–≤ –ø—Ä–∏ —Ä–∞—Å—à–∞—Ä–∏–≤–∞–Ω–∏–∏ –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∫–∞—Ä—Ç–∏–Ω–∫–∞.",
        "fix": "–ü—Ä–æ–ø–∏—Å–∞—Ç—å og:title, og:description, og:image –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.",
        "effort": "medium",
    },
    "missing_charset": {
        "title": "–£–∫–∞–∑–∞—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É UTF-8 –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö",
        "impact": "–ë–µ–∑ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –±—Ä–∞—É–∑–µ—Ä –º–æ–∂–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã.",
        "fix": '–î–æ–±–∞–≤–∏—Ç—å <meta charset="UTF-8"> –≤ –Ω–∞—á–∞–ª–æ <head>.',
        "effort": "quick",
    },
    "mixed_content": {
        "title": "–£—Å—Ç—Ä–∞–Ω–∏—Ç—å —Å–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö",
        "impact": "HTTPS-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞–≥—Ä—É–∂–∞—é—â–∏–µ HTTP-—Ä–µ—Å—É—Ä—Å—ã –ø–æ–º–µ—á–∞—é—Ç—Å—è –∫–∞–∫ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ.",
        "fix": "–û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ—Å—É—Ä—Å—ã (–∫–∞—Ä—Ç–∏–Ω–∫–∏, —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏) –Ω–∞ HTTPS-–≤–µ—Ä—Å–∏–∏.",
        "effort": "medium",
    },
    "heading_hierarchy": {
        "title": "–ò—Å–ø—Ä–∞–≤–∏—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—é –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ {count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö",
        "impact": "–ù–∞—Ä—É—à–µ–Ω–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—è (H3 –ø–µ—Ä–µ–¥ H2) –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞.",
        "fix": "–£–ø–æ—Ä—è–¥–æ—á–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: H1 -> H2 -> H3, –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —É—Ä–æ–≤–Ω–µ–π.",
        "effort": "medium",
    },
    "noindex_linked": {
        "title": "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å {count} —Å—Ç—Ä–∞–Ω–∏—Ü —Å noindex, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –≤–µ–¥—É—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏",
        "impact": "–°—Å—ã–ª–∫–∏ –Ω–∞ noindex-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç—Ä–∞—Ç—è—Ç –∫—Ä–∞—É–ª–∏–Ω–≥–æ–≤—ã–π –±—é–¥–∂–µ—Ç –≤–ø—É—Å—Ç—É—é.",
        "fix": "–õ–∏–±–æ —É–±—Ä–∞—Ç—å noindex, –ª–∏–±–æ —É–¥–∞–ª–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —ç—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.",
        "effort": "medium",
    },
    "no_hreflang": {
        "title": "–ù–∞—Å—Ç—Ä–æ–∏—Ç—å hreflang –¥–ª—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ–≥–æ —Å–∞–π—Ç–∞",
        "impact": "–ë–µ–∑ hreflang –Ø–Ω–¥–µ–∫—Å –∏ Google –º–æ–≥—É—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –≤–µ—Ä—Å–∏—é.",
        "fix": "–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥–∏ hreflang –¥–ª—è –∫–∞–∂–¥–æ–π —è–∑—ã–∫–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.",
        "effort": "complex",
    },
    "yandex_clean_param": {
        "title": "–î–æ–±–∞–≤–∏—Ç—å Clean-param –≤ robots.txt –¥–ª—è –Ø–Ω–¥–µ–∫—Å–∞",
        "impact": "–ë–µ–∑ Clean-param –Ø–Ω–¥–µ–∫—Å –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥—É–±–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü —Å UTM-–º–µ—Ç–∫–∞–º–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏.",
        "fix": "–î–æ–±–∞–≤–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–∏–≤—É Clean-param –≤ robots.txt –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ utm_*, sort, filter –∏ —Ç.–¥.",
        "effort": "quick",
    },
    "dead_end_pages": {
        "title": "–ò—Å–ø—Ä–∞–≤–∏—Ç—å {count} —Ç—É–ø–∏–∫–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫",
        "impact": "–¢—É–ø–∏–∫–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –ø–µ—Ä–µ–¥–∞—é—Ç —Å—Å—ã–ª–æ—á–Ω—ã–π –≤–µ—Å –¥–∞–ª—å—à–µ –ø–æ —Å–∞–π—Ç—É, —É—Ö—É–¥—à–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ PageRank –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –≥–ª—É–±–æ–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.",
        "fix": "–î–æ–±–∞–≤–∏—Ç—å –±–ª–æ–∫–∏ ¬´–ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏¬ª, —Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏ –∏–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —Å—Å—ã–ª–æ—á–Ω–æ–≥–æ –≤–µ—Å–∞.",
        "effort": "medium",
    },
    "low_inlinks": {
        "title": "–£—Å–∏–ª–∏—Ç—å –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫—É –¥–ª—è {count} —Å–ª–∞–±–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü",
        "impact": "–°—Ç—Ä–∞–Ω–∏—Ü—ã —Å <2 –≤—Ö–æ–¥—è—â–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –ø–æ–ª—É—á–∞—é—Ç –º–∞–ª–æ —Å—Å—ã–ª–æ—á–Ω–æ–≥–æ –≤–µ—Å–∞ –∏ —Ö—É–∂–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É—é—Ç—Å—è Google –∏ –Ø–Ω–¥–µ–∫—Å–æ–º.",
        "fix": "–î–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ —ç—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤, –±–ª–æ–≥–∞ –∏–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏.",
        "effort": "medium",
    },
    "canonical_chain": {
        "title": "–£—Å—Ç—Ä–∞–Ω–∏—Ç—å {count} —Ü–µ–ø–æ—á–µ–∫ –∫–∞–Ω–æ–Ω–∏–∫–∞–ª–æ–≤",
        "impact": "–¶–µ–ø–æ—á–∫–∏ canonical A‚ÜíB‚ÜíC –∑–∞–º–µ–¥–ª—è—é—Ç –ø–µ—Ä–µ–¥–∞—á—É —Å–∏–≥–Ω–∞–ª–æ–≤ –∏ –º–æ–≥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞–º–∏.",
        "fix": "–ö–∞–∂–¥–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–æ–ª–∂–Ω–∞ —É–∫–∞–∑—ã–≤–∞—Ç—å canonical –Ω–∞–ø—Ä—è–º—É—é –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–π URL, –±–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∑–≤–µ–Ω—å–µ–≤.",
        "effort": "quick",
    },
    "canonical_sitemap_conflict": {
        "title": "–ò—Å–ø—Ä–∞–≤–∏—Ç—å {count} –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ canonical/sitemap",
        "impact": "–°—Ç—Ä–∞–Ω–∏—Ü–∞ –≤–∫–ª—é—á–µ–Ω–∞ –≤ sitemap, –Ω–æ canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–π URL ‚Äî –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–π —Å–∏–≥–Ω–∞–ª –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤.",
        "fix": "–õ–∏–±–æ —É–±—Ä–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑ sitemap, –ª–∏–±–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å canonical –Ω–∞ self-referencing.",
        "effort": "quick",
    },
    "schema_incomplete": {
        "title": "–ó–∞–ø–æ–ª–Ω–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è Schema.org –¥–ª—è {count} —Å—Ç—Ä–∞–Ω–∏—Ü",
        "impact": "–ù–µ–ø–æ–ª–Ω–∞—è Schema-—Ä–∞–∑–º–µ—Ç–∫–∞ –Ω–µ –¥–∞—ë—Ç rich-results –≤ –≤—ã–¥–∞—á–µ Google. –Ø–Ω–¥–µ–∫—Å —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Schema –¥–ª—è —Å–Ω–∏–ø–ø–µ—Ç–æ–≤.",
        "fix": "–î–æ–±–∞–≤–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (@type-specific) –≤ JSON-LD —Ä–∞–∑–º–µ—Ç–∫—É.",
        "effort": "medium",
    },
}

ERROR_MESSAGES_RU = {
    "invalid_url": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∞–¥—Ä–µ—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å https:// –∏–ª–∏ http://",
    "connection_error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ —Å–∞–π—Ç—É. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–¥—Ä–µ—Å –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å.",
    "timeout": "–°–∞–π—Ç –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ 15 —Å–µ–∫—É–Ω–¥. –í–æ–∑–º–æ–∂–Ω–æ, –æ–Ω –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω.",
    "ssl_error": "–û—à–∏–±–∫–∞ SSL-—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞. –°–∞–π—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–±–µ–∑–æ–ø–∞—Å–µ–Ω.",
    "blocked": "–°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å (403/429). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å –∑–∞–¥–µ—Ä–∂–∫—É.",
    "cloudflare": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ Cloudflare. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–æ–π—Ç–∏ –ø—Ä–æ–≤–µ—Ä–∫—É.",
    "no_pages": "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –°–∞–π—Ç –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å JS-–Ω–∞–≤–∏–≥–∞—Ü–∏—é.",
    "empty_response": "–°–∞–π—Ç –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è –ª–∏ –æ–Ω –≤ –±—Ä–∞—É–∑–µ—Ä–µ.",
    "busy": "–°–µ—Ä–≤–∏—Å —Å–µ–π—á–∞—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç –¥—Ä—É–≥–æ–π –∞—É–¥–∏—Ç. –ü–æ–¥–æ–∂–¥–∏—Ç–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–æ–≤–∞.",
}

# JS framework detection patterns
JS_FRAMEWORKS = {
    "Tilda": ["tilda", "t-body", "t-records", "t-store"],
    "Wix": ["wix.com", "_wix", "wixsite"],
    "React": ["__NEXT_DATA__", "_next/", "react-root", "__next"],
    "Vue.js": ["__vue__", "v-app", "nuxt"],
    "Angular": ["ng-version", "ng-app"],
    "WordPress": ["wp-content", "wp-includes", "wordpress"],
    "Bitrix": ["bitrix", "bx-panel"],
    "1C-Bitrix": ["1c-bitrix"],
    "Joomla": ["joomla", "/components/com_"],
    "Drupal": ["drupal", "sites/default/files"],
    "Shopify": ["shopify", "cdn.shopify.com"],
    "Squarespace": ["squarespace"],
    "Webflow": ["webflow"],
    "Modx": ["modx", "assets/components"],
}

COLORS = {
    "primary": "#6366F1",
    "primary_light": "#A5B4FC",
    "primary_dark": "#4338CA",
    "bg_main": "#0F172A",
    "bg_card": "#1E293B",
    "bg_card_hover": "#334155",
    "bg_sidebar": "#0F172A",
    "bg_input": "#1E293B",
    "text_primary": "#F8FAFC",
    "text_secondary": "#94A3B8",
    "text_muted": "#64748B",
    "critical": "#EF4444",
    "critical_bg": "#7F1D1D",
    "warning": "#F59E0B",
    "warning_bg": "#78350F",
    "success": "#10B981",
    "success_bg": "#064E3B",
    "info": "#3B82F6",
    "info_bg": "#1E3A5F",
    "border": "#334155",
    "border_light": "#475569",
    "progress_track": "#1E293B",
    "progress_fill": "#6366F1",
    "chart_palette": [
        "#6366F1", "#10B981", "#F59E0B", "#EF4444", "#3B82F6",
        "#8B5CF6", "#EC4899", "#14B8A6", "#F97316", "#06B6D4",
    ],
}

ICONS = {
    "score": "üèÜ",
    "pages": "üìÑ",
    "critical": "üî¥",
    "warning": "üü°",
    "info": "üîµ",
    "success": "üü¢",
    "speed": "‚ö°",
    "link": "üîó",
    "image": "üñºÔ∏è",
    "code": "üîß",
    "content": "üìù",
    "security": "üîí",
    "export": "üì•",
    "search": "üîç",
    "chart": "üìä",
    "warning_sign": "‚ö†Ô∏è",
    "rocket": "üöÄ",
    "target": "üéØ",
    "clock": "üïê",
    "globe": "üåê",
}

EFFORT_LABELS = {
    "quick": "–ë—ã—Å—Ç—Ä–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (~15 –º–∏–Ω)",
    "medium": "–°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å (~1 —á–∞—Å)",
    "complex": "–°–ª–æ–∂–Ω–æ–µ (–Ω—É–∂–µ–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫)",
}

PLOTLY_LAYOUT = dict(
    template="plotly_dark",
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
    font=dict(family="Inter, sans-serif", color="#CBD5E1"),
    margin=dict(l=20, r=20, t=40, b=20),
)

LOG_BUFFER: deque[str] = deque(maxlen=200)


class BufferLogHandler(logging.Handler):
    """–ö–æ–ø–∏—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–∫–∞–∑–∞ –≤ Streamlit-—Å–∞–π–¥–±–∞—Ä–µ."""

    def emit(self, record: logging.LogRecord) -> None:
        message = self.format(record)
        if record.levelno >= logging.WARNING:
            LOG_BUFFER.append(message)


if not any(isinstance(h, BufferLogHandler) for h in logger.handlers):
    mem_handler = BufferLogHandler()
    mem_handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
    logger.addHandler(mem_handler)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


@dataclass
class AuditRuntimeGuard:
    """Timestamp-based guard: no Lock ‚Äî no deadlock on Streamlit rerun.

    The previous Lock-based approach caused permanent lockout when
    Streamlit reran the script mid-crawl (Lock.release never called).
    Now we use a timestamp with automatic expiry.
    """

    _lock: threading.Lock = field(default_factory=threading.Lock)
    active_session_id: str = ""
    started_at: float = 0.0
    max_pages_hint: int = 50  # used to compute TTL

    def try_acquire(self, session_id: str, max_pages: int = 50) -> bool:
        """Try to start an audit. Returns True if acquired."""
        with self._lock:
            now = time.time()
            ttl = max(120, max_pages * 3 + 180)  # generous TTL
            if self.active_session_id and self.active_session_id != session_id:
                if (now - self.started_at) < ttl:
                    return False  # another session is actively crawling
                # expired ‚Äî reclaim
            self.active_session_id = session_id
            self.started_at = now
            self.max_pages_hint = max_pages
            return True

    def release(self, session_id: str) -> None:
        """Release the audit slot."""
        with self._lock:
            if self.active_session_id == session_id:
                self.active_session_id = ""
                self.started_at = 0.0

    def is_busy(self, session_id: str) -> bool:
        """Check if another session is actively auditing."""
        with self._lock:
            if not self.active_session_id:
                return False
            if self.active_session_id == session_id:
                return False
            ttl = max(120, self.max_pages_hint * 3 + 180)
            if (time.time() - self.started_at) >= ttl:
                self.active_session_id = ""
                self.started_at = 0.0
                return False
            return True


@st.cache_resource(show_spinner=False)
def get_runtime_guard() -> AuditRuntimeGuard:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç singleton-guard –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å Streamlit."""
    return AuditRuntimeGuard()


@dataclass
class AuthSessionGuard:
    """–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–∞—è —Å–µ—Å—Å–∏—è: –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å."""

    lock: threading.Lock = field(default_factory=threading.Lock)
    active_session_id: str = ""
    active_username: str = ""
    last_seen: float = 0.0


@st.cache_resource(show_spinner=False)
def get_auth_guard() -> AuthSessionGuard:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç singleton-guard –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å Streamlit."""
    return AuthSessionGuard()


# === DATA CLASSES ===

@dataclass
class PageIssue:
    severity: str       # critical, warning, info
    category: str       # technical, content, links, images, structured_data, security
    code: str           # machine key e.g. missing_h1
    message: str        # human-readable Russian description
    current_value: str = ""
    expected_value: str = ""

    def to_dict(self) -> dict:
        return {
            "severity": self.severity,
            "category": self.category,
            "code": self.code,
            "message": self.message,
            "currentValue": self.current_value,
            "expectedValue": self.expected_value,
        }


@dataclass
class PageResult:
    url: str
    status_code: int = 0
    ttfb: float = 0.0
    content_type: str = ""
    content_length: int = 0
    title: str = ""
    title_length: int = 0
    description: str = ""
    description_length: int = 0
    h1_list: List[str] = field(default_factory=list)
    h1_count: int = 0
    headings: List[Dict[str, str]] = field(default_factory=list)
    word_count: int = 0
    text_html_ratio: float = 0.0
    content_hash: str = ""
    canonical: str = ""
    canonical_status: str = ""  # ok, missing, mismatch, error
    has_meta_robots: bool = False
    meta_robots: str = ""
    x_robots_tag: str = ""
    is_indexable: bool = True
    internal_links: int = 0
    external_links: int = 0
    internal_link_urls: List[str] = field(default_factory=list)
    external_link_urls: List[str] = field(default_factory=list)
    empty_href_links: int = 0
    internal_links_nofollow: int = 0
    broken_links_on_page: List[str] = field(default_factory=list)
    broken_internal_links: int = 0
    broken_external_links: int = 0
    inlink_count: int = 0          # incoming internal links
    is_dead_end: bool = False       # no outgoing internal links
    internal_pagerank: float = 0.0  # simplified PageRank score
    images_total: int = 0
    images_missing_alt: int = 0
    images_empty_alt: int = 0
    images_missing_dimensions: int = 0
    images_broken: int = 0
    images_large: int = 0
    has_schema: bool = False
    schema_types: List[str] = field(default_factory=list)
    has_microdata: bool = False
    has_og: bool = False
    og_tags: Dict[str, str] = field(default_factory=dict)
    has_twitter_card: bool = False
    has_viewport: bool = False
    has_charset: bool = False
    charset_value: str = ""
    has_mixed_content: bool = False
    redirect_chain: List[str] = field(default_factory=list)
    redirect_type: int = 0
    response_headers: Dict[str, str] = field(default_factory=dict)
    has_hsts: bool = False
    crawl_depth: int = 0
    issues: List[PageIssue] = field(default_factory=list)
    error_message: str = ""
    detected_framework: str = ""
    has_lang: bool = False
    lang_value: str = ""
    has_hreflang: bool = False
    hreflang_tags: List[Dict[str, str]] = field(default_factory=list)
    content_text: str = ""
    raw_html: str = ""
    canonical_absolute: str = ""
    canonical_target_status: int = 0
    js_render_warning: bool = False

    def to_dict(self) -> dict:
        return {
            "url": self.url,
            "statusCode": self.status_code,
            "ttfb": round(self.ttfb, 3),
            "contentType": self.content_type,
            "title": self.title,
            "titleLength": self.title_length,
            "description": self.description,
            "descriptionLength": self.description_length,
            "h1List": self.h1_list,
            "h1Count": self.h1_count,
            "headings": self.headings,
            "wordCount": self.word_count,
            "textHtmlRatio": round(self.text_html_ratio, 1),
            "contentHash": self.content_hash,
            "canonical": self.canonical,
            "canonicalStatus": self.canonical_status,
            "isIndexable": self.is_indexable,
            "metaRobots": self.meta_robots,
            "xRobotsTag": self.x_robots_tag,
            "internalLinks": self.internal_links,
            "externalLinks": self.external_links,
            "internalLinkUrls": self.internal_link_urls[:200],
            "externalLinkUrls": self.external_link_urls[:200],
            "emptyHrefLinks": self.empty_href_links,
            "internalLinksNofollow": self.internal_links_nofollow,
            "brokenLinksOnPage": self.broken_links_on_page,
            "brokenInternalLinks": self.broken_internal_links,
            "brokenExternalLinks": self.broken_external_links,
            "imagesTotal": self.images_total,
            "imagesMissingAlt": self.images_missing_alt,
            "imagesEmptyAlt": self.images_empty_alt,
            "imagesMissingDimensions": self.images_missing_dimensions,
            "imagesBroken": self.images_broken,
            "imagesLarge": self.images_large,
            "hasSchema": self.has_schema,
            "schemaTypes": self.schema_types,
            "hasMicrodata": self.has_microdata,
            "hasOg": self.has_og,
            "ogTags": self.og_tags,
            "hasTwitterCard": self.has_twitter_card,
            "hasViewport": self.has_viewport,
            "hasCharset": self.has_charset,
            "hasMixedContent": self.has_mixed_content,
            "redirectChain": self.redirect_chain,
            "hasHsts": self.has_hsts,
            "crawlDepth": self.crawl_depth,
            "issues": [i.to_dict() for i in self.issues],
            "errorMessage": self.error_message,
            "detectedFramework": self.detected_framework,
            "hasLang": self.has_lang,
            "langValue": self.lang_value,
            "hasHreflang": self.has_hreflang,
            "canonicalAbsolute": self.canonical_absolute,
            "canonicalTargetStatus": self.canonical_target_status,
            "jsRenderWarning": self.js_render_warning,
        }


@dataclass
class SiteAuditResult:
    base_url: str
    domain: str
    pages: Dict[str, PageResult] = field(default_factory=dict)
    duration: float = 0.0
    total_scanned: int = 0
    sitemap_urls: Set[str] = field(default_factory=set)
    sitemap_entries: List[Dict[str, str]] = field(default_factory=list)
    sitemap_raw: str = ""
    robots_txt_content: str = ""
    robots_rules: List[Dict[str, str]] = field(default_factory=list)
    robots_sitemaps: List[str] = field(default_factory=list)
    robots_has_host: bool = False
    robots_host_value: str = ""
    robots_has_clean_param: bool = False
    robots_linked_blocked: List[str] = field(default_factory=list)
    ssl_valid: bool = True
    ssl_error: str = ""
    tls_version: str = ""
    http_to_https: bool = False
    www_consistency: str = ""  # "www", "non-www", "inconsistent", "unknown"
    homepage_status_code: int = 0
    homepage_ttfb: float = 0.0
    homepage_headers: Dict[str, str] = field(default_factory=dict)
    detected_frameworks: List[str] = field(default_factory=list)
    has_llms_txt: bool = False
    llms_txt_content: str = ""
    # Cross-page analysis results (filled after crawl)
    duplicate_titles: List[Dict] = field(default_factory=list)
    duplicate_descriptions: List[Dict] = field(default_factory=list)
    duplicate_content: List[Dict] = field(default_factory=list)
    orphan_pages: List[str] = field(default_factory=list)
    deep_pages: List[Dict] = field(default_factory=list)
    zero_inlink_pages: List[str] = field(default_factory=list)
    noindex_linked_pages: List[str] = field(default_factory=list)
    redirect_map: List[Dict] = field(default_factory=list)
    broken_link_map: List[Dict] = field(default_factory=list)
    url_pattern_duplicates: List[Dict[str, Any]] = field(default_factory=list)
    thin_content_clusters: List[Dict[str, Any]] = field(default_factory=list)
    sitemap_vs_crawled: Dict[str, List[str]] = field(default_factory=dict)
    no_structured_data: bool = False
    dead_end_pages: List[str] = field(default_factory=list)
    # Interlinking stats
    avg_inlinks: float = 0.0
    max_inlinks: int = 0
    median_inlinks: float = 0.0
    pages_low_inlinks: List[str] = field(default_factory=list)  # <2 inlinks (excl. homepage)
    canonical_chains: List[Dict] = field(default_factory=list)
    canonical_sitemap_conflicts: List[Dict] = field(default_factory=list)
    schema_validation_issues: List[Dict] = field(default_factory=list)
    score_explanation: Dict[str, Any] = field(default_factory=dict)
    # Scoring
    health_score: int = 0
    category_scores: Dict[str, int] = field(default_factory=dict)
    # Recommendations
    recommendations: List[Dict] = field(default_factory=list)
    # Stats
    status_codes: Dict[str, int] = field(default_factory=dict)
    issues_by_severity: Dict[str, int] = field(default_factory=dict)
    issues_by_category: Dict[str, Dict[str, int]] = field(default_factory=dict)


# === CRAWLER ENGINE ===

class CrawlerEngine:
    """BFS-based web crawler with parallel fetching."""

    def __init__(
        self,
        start_url: str,
        max_pages: int = 50,
        max_depth: int = 5,
        crawl_delay: float = 0.3,
        respect_robots: bool = True,
        check_external: bool = False,
    ):
        parsed = urlparse(start_url)
        if not parsed.scheme:
            start_url = "https://" + start_url
            parsed = urlparse(start_url)

        self.start_url = start_url
        self.domain = parsed.netloc
        self.scheme = parsed.scheme
        self.base_origin = f"{parsed.scheme}://{parsed.netloc}"
        self.max_pages = max_pages
        self.max_depth = max_depth
        self.crawl_delay = crawl_delay
        self.respect_robots = respect_robots
        self.check_external = check_external
        self.max_workers = MAX_WORKERS

        self.visited: Set[str] = set()
        self.queue: deque = deque()  # deque for O(1) popleft in BFS
        self.results: Dict[str, PageResult] = {}
        self.all_discovered_links: Set[str] = set()
        self.internal_link_targets: Dict[str, Set[str]] = defaultdict(set)  # target -> set of source pages
        self.external_links_found: Set[str] = set()
        self.page_outgoing_links: Dict[str, Dict[str, Set[str]]] = defaultdict(
            lambda: {"internal": set(), "external": set(), "blocked": set()}
        )
        self.raw_url_variants: Dict[str, Set[str]] = defaultdict(set)
        self.resource_info_cache: Dict[str, Tuple[int, int]] = {}
        self.resource_cache_lock = threading.Lock()
        self._critical_count = 0  # running counter instead of O(N*M) recount

        self.session = requests.Session()
        retry = Retry(
            total=2,
            read=2,
            connect=2,
            backoff_factor=0.7,
            status_forcelist=[500, 502, 503, 504],
            allowed_methods=frozenset(["GET", "HEAD"]),
            raise_on_status=False,
        )
        pool_size = max(10, self.max_workers * 4)
        adapter = HTTPAdapter(max_retries=retry, pool_connections=pool_size, pool_maxsize=pool_size)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        ua_string = _UA.random if _UA else (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        )
        self.session.headers.update({
            "User-Agent": ua_string,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
        })
        self.session.max_redirects = MAX_REDIRECTS

        self.robots_parser: Optional[RobotFileParser] = None
        self.robots_txt_content = ""
        self.robots_rules: List[Dict[str, str]] = []
        self.robots_sitemaps: List[str] = []
        self.robots_has_host = False
        self.robots_host_value = ""
        self.robots_has_clean_param = False
        self.sitemap_urls: Set[str] = set()
        self.sitemap_entries: List[Dict[str, str]] = []
        self.sitemap_raw = ""

        self.ssl_valid = True
        self.ssl_error = ""
        self.tls_version = ""
        self.http_to_https = False
        self.www_consistency = "unknown"
        self.homepage_status_code = 0
        self.homepage_ttfb = 0.0
        self.homepage_headers: Dict[str, str] = {}
        self.has_llms_txt = False
        self.llms_txt_content = ""
        self.linked_but_robots_blocked: Set[str] = set()

        self._progress_callback = None
        self._stop_flag = False
        self._url_param_counts: Dict[str, int] = defaultdict(int)

    def _request_headers(self) -> Dict[str, str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å —Ä–æ—Ç–∏—Ä—É–µ–º—ã–º User-Agent."""
        headers = dict(self.session.headers)
        try:
            if _UA:
                headers["User-Agent"] = _UA.random
        except Exception:
            pass
        return headers

    def normalize_url(self, url: str) -> str:
        """Normalize URL: strip fragment, trailing slash (except root), sort query params."""
        try:
            parsed = urlparse(url)
            scheme = parsed.scheme.lower() if parsed.scheme else self.scheme
            netloc = parsed.netloc.lower() if parsed.netloc else self.domain
            path = parsed.path.rstrip("/") or "/"
            path = quote(path, safe="/%:@")
            # Sort query params for deduplication
            qs = parse_qs(parsed.query, keep_blank_values=True)
            sorted_qs = urlencode(sorted(qs.items()), doseq=True)
            return urlunparse((scheme, netloc, path, "", sorted_qs, ""))
        except Exception:
            return url

    def is_internal(self, url: str) -> bool:
        try:
            parsed = urlparse(url)
            if parsed.netloc == "":
                return True
            netloc = parsed.netloc.lower()
            domain = self.domain.lower()
            return netloc == domain or netloc.replace("www.", "") == domain.replace("www.", "")
        except Exception:
            return False

    def is_crawlable_url(self, url: str) -> bool:
        """Check if URL should be crawled (skip non-HTML resources)."""
        parsed = urlparse(url)
        path_lower = parsed.path.lower()
        skip_extensions = {
            ".pdf", ".jpg", ".jpeg", ".png", ".gif", ".svg", ".webp", ".ico",
            ".css", ".js", ".json", ".xml", ".txt", ".zip", ".rar", ".gz",
            ".mp3", ".mp4", ".avi", ".mov", ".wmv", ".doc", ".docx", ".xls",
            ".xlsx", ".ppt", ".pptx", ".woff", ".woff2", ".ttf", ".eot",
        }
        for ext in skip_extensions:
            if path_lower.endswith(ext):
                return False
        return parsed.scheme in ("http", "https", "")

    def is_trap_url(self, url: str) -> bool:
        """Detect infinite crawl traps."""
        for pattern in TRAP_PATTERNS:
            if re.search(pattern, url):
                base = urlparse(url).path.split("?")[0]
                self._url_param_counts[base] += 1
                if self._url_param_counts[base] > 3:
                    return True
        return False

    # --- Pre-crawl checks ---

    def fetch_robots_txt(self) -> None:
        """Fetch and parse robots.txt."""
        robots_url = f"{self.base_origin}/robots.txt"
        try:
            resp = self.session.get(robots_url, timeout=10, headers=self._request_headers())
            if resp.status_code == 200:
                self.robots_txt_content = resp.text
                self._parse_robots_txt(resp.text)
                # Standard RobotFileParser
                self.robots_parser = RobotFileParser()
                self.robots_parser.set_url(robots_url)
                self.robots_parser.parse(resp.text.splitlines())
        except Exception as e:
            logger.warning(f"Failed to fetch robots.txt: {e}")

    def _parse_robots_txt(self, content: str) -> None:
        """Extract rules, sitemaps, Yandex-specific directives from robots.txt."""
        current_ua = "*"
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            parts = line.split(":", 1)
            if len(parts) != 2:
                continue
            directive = parts[0].strip().lower()
            value = parts[1].strip()

            if directive == "user-agent":
                current_ua = value
            elif directive in ("allow", "disallow"):
                self.robots_rules.append({
                    "userAgent": current_ua,
                    "directive": directive.capitalize(),
                    "path": value,
                })
            elif directive == "sitemap":
                self.robots_sitemaps.append(value)
            elif directive == "host":
                self.robots_has_host = True
                self.robots_host_value = value
            elif directive == "clean-param":
                self.robots_has_clean_param = True

    def fetch_sitemap(self) -> None:
        """Fetch and parse sitemap.xml (follows sitemap index)."""
        sitemap_urls_to_check = [f"{self.base_origin}/sitemap.xml"]
        sitemap_urls_to_check.extend(self.robots_sitemaps)
        seen_sitemaps = set()

        for sitemap_url in sitemap_urls_to_check:
            if sitemap_url in seen_sitemaps:
                continue
            seen_sitemaps.add(sitemap_url)
            try:
                resp = self.session.get(sitemap_url, timeout=10, headers=self._request_headers())
                if resp.status_code != 200:
                    continue
                content = resp.content
                if not self.sitemap_raw:
                    self.sitemap_raw = resp.text[:5000]

                soup = BeautifulSoup(content, "lxml-xml")

                # Check for sitemap index
                sitemapindex_locs = soup.find_all("sitemap")
                if sitemapindex_locs:
                    for sm in sitemapindex_locs:
                        loc = sm.find("loc")
                        if loc and loc.text not in seen_sitemaps:
                            sitemap_urls_to_check.append(loc.text.strip())
                    continue

                # Regular sitemap
                for url_tag in soup.find_all("url"):
                    loc = url_tag.find("loc")
                    if loc:
                        normalized = self.normalize_url(loc.text.strip())
                        self.sitemap_urls.add(normalized)
                        lastmod_tag = url_tag.find("lastmod")
                        self.sitemap_entries.append(
                            {
                                "url": normalized,
                                "lastmod": (lastmod_tag.text.strip() if lastmod_tag and lastmod_tag.text else ""),
                            }
                        )
            except Exception as e:
                logger.warning(f"Failed to parse sitemap {sitemap_url}: {e}")

        # Deduplicate entries while preserving first seen lastmod
        unique_entries: Dict[str, Dict[str, str]] = {}
        for item in self.sitemap_entries:
            if item["url"] not in unique_entries:
                unique_entries[item["url"]] = item
        self.sitemap_entries = list(unique_entries.values())

    def check_ssl(self) -> None:
        """Check SSL certificate validity."""
        try:
            requests.get(f"https://{self.domain}", timeout=10, verify=True)
            self.ssl_valid = True
            try:
                context = ssl.create_default_context()
                with socket.create_connection((self.domain, 443), timeout=8) as sock:
                    with context.wrap_socket(sock, server_hostname=self.domain) as secure_sock:
                        self.tls_version = secure_sock.version() or ""
            except Exception:
                self.tls_version = ""
        except requests.exceptions.SSLError as e:
            self.ssl_valid = False
            self.ssl_error = str(e)[:200]
        except Exception:
            pass

    def check_http_redirect(self) -> None:
        """Check if HTTP redirects to HTTPS."""
        try:
            resp = requests.get(
                f"http://{self.domain}",
                timeout=10,
                allow_redirects=True,
                verify=False,
                headers=self._request_headers(),
            )
            final_url = resp.url
            self.http_to_https = urlparse(final_url).scheme == "https"
        except Exception:
            pass

    def check_www_consistency(self) -> None:
        """Check www vs non-www consistency."""
        try:
            domain_parts = self.domain.split(".")
            if self.domain.startswith("www."):
                alt_domain = self.domain[4:]
            else:
                alt_domain = "www." + self.domain

            resp = requests.get(
                f"https://{alt_domain}",
                timeout=10,
                allow_redirects=True,
                verify=False,
                headers=self._request_headers(),
            )
            final_domain = urlparse(resp.url).netloc

            if final_domain == self.domain:
                self.www_consistency = "www" if self.domain.startswith("www.") else "non-www"
            else:
                self.www_consistency = "inconsistent"
        except Exception:
            self.www_consistency = "unknown"

    def check_llms_txt(self) -> None:
        """Check if /llms.txt exists."""
        try:
            resp = self.session.get(f"{self.base_origin}/llms.txt", timeout=10, headers=self._request_headers())
            if resp.status_code == 200 and len(resp.text) > 0:
                self.has_llms_txt = True
                self.llms_txt_content = resp.text[:2000]
        except Exception:
            pass

    def check_homepage(self) -> None:
        """–§–∏–∫—Å–∏—Ä—É–µ—Ç –∫–æ–¥, TTFB –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        try:
            resp = self.session.get(
                self.start_url,
                timeout=15,
                allow_redirects=True,
                headers=self._request_headers(),
            )
            self.homepage_status_code = resp.status_code
            self.homepage_ttfb = resp.elapsed.total_seconds()
            self.homepage_headers = dict(resp.headers)
        except Exception as exc:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: %s", exc)

    def run_pre_checks(self, callback=None) -> None:
        """Run all pre-crawl checks."""
        steps = [
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ robots.txt...", self.fetch_robots_txt),
            ("–ó–∞–≥—Ä—É–∑–∫–∞ sitemap.xml...", self.fetch_sitemap),
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ SSL...", self.check_ssl),
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ HTTP -> HTTPS...", self.check_http_redirect),
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ www-–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏...", self.check_www_consistency),
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ llms.txt...", self.check_llms_txt),
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...", self.check_homepage),
        ]
        for msg, func in steps:
            if callback:
                callback({"type": "pre_check", "message": msg})
            try:
                func()
            except Exception as e:
                logger.error(f"Pre-check error ({msg}): {e}")

    # --- Main crawl ---

    def crawl(self, progress_callback=None) -> SiteAuditResult:
        """Run BFS crawl and return full audit result."""
        self._progress_callback = progress_callback

        # Pre-checks
        self.run_pre_checks(progress_callback)

        # Start BFS
        start_norm = self.normalize_url(self.start_url)
        self.queue.append((self.start_url, 0))
        self.visited.add(start_norm)

        start_time = time.time()

        while self.queue and len(self.results) < self.max_pages and not self._stop_flag:
            # Take batch from queue
            batch = []
            while self.queue and len(batch) < self.max_workers and (len(self.results) + len(batch)) < self.max_pages:
                batch.append(self.queue.popleft())

            if not batch:
                break

            # Process batch in parallel
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {}
                for url, depth in batch:
                    futures[executor.submit(self._fetch_and_analyze, url, depth)] = (url, depth)

                for future in as_completed(futures):
                    url, depth = futures[future]
                    try:
                        page_result, discovered_internal, discovered_external = future.result()
                        if page_result:
                            source_key = self.normalize_url(page_result.url or url)
                            page_result.url = source_key
                            self.results[source_key] = page_result

                            # Update running critical counter (O(K) per page, not O(N*M))
                            if any(i.severity == "critical" for i in page_result.issues):
                                self._critical_count += 1

                            # Report progress
                            if progress_callback:
                                progress_callback({
                                    "type": "page_done",
                                    "url": source_key,
                                    "statusCode": page_result.status_code,
                                    "ttfb": round(page_result.ttfb, 2),
                                    "pagesScanned": len(self.results),
                                    "totalDiscovered": len(self.visited),
                                    "queueSize": len(self.queue),
                                    "errorsCount": self._critical_count,
                                })

                            for link in discovered_internal:
                                norm = self.normalize_url(link)
                                self.page_outgoing_links[source_key]["internal"].add(norm)
                                self.raw_url_variants[norm].add(link)
                                self.internal_link_targets[norm].add(source_key)

                                if depth >= self.max_depth:
                                    continue
                                if norm in self.visited or not self.is_internal(link):
                                    continue
                                if not self.is_crawlable_url(link) or self.is_trap_url(link):
                                    continue
                                if self.robots_parser and not self.robots_parser.can_fetch("*", link):
                                    self.linked_but_robots_blocked.add(norm)
                                    self.page_outgoing_links[source_key]["blocked"].add(norm)
                                    if self.respect_robots:
                                        continue
                                self.visited.add(norm)
                                self.queue.append((link, depth + 1))

                            for external_link in discovered_external:
                                self.external_links_found.add(external_link)
                                self.page_outgoing_links[source_key]["external"].add(external_link)

                    except Exception as e:
                        logger.error(f"Error processing {url}: {e}")
                        err_result = PageResult(url=url, error_message=str(e)[:200])
                        err_result.issues.append(PageIssue(
                            "critical", "technical", "crawl_error",
                            f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: {str(e)[:100]}",
                        ))
                        self.results[self.normalize_url(url)] = err_result
                        if progress_callback:
                            progress_callback({
                                "type": "page_error",
                                "url": self.normalize_url(url),
                                "error": str(e)[:100],
                                "pagesScanned": len(self.results),
                                "totalDiscovered": len(self.visited),
                                "queueSize": len(self.queue),
                            })

            # Crawl delay
            time.sleep(self.crawl_delay)

        unresolved_internal_status = self._resolve_uncrawled_internal_statuses()
        canonical_status_cache = self._resolve_canonical_statuses()
        external_status_cache = {}
        if self.check_external and self.external_links_found:
            external_status_cache = self._check_external_links(progress_callback)

        elapsed = time.time() - start_time

        # Build result
        result = SiteAuditResult(
            base_url=self.start_url,
            domain=self.domain,
            pages=self.results,
            duration=round(elapsed, 2),
            total_scanned=len(self.results),
            sitemap_urls=self.sitemap_urls,
            sitemap_entries=self.sitemap_entries,
            sitemap_raw=self.sitemap_raw,
            robots_txt_content=self.robots_txt_content,
            robots_rules=self.robots_rules,
            robots_sitemaps=self.robots_sitemaps,
            robots_has_host=self.robots_has_host,
            robots_host_value=self.robots_host_value,
            robots_has_clean_param=self.robots_has_clean_param,
            robots_linked_blocked=sorted(self.linked_but_robots_blocked),
            ssl_valid=self.ssl_valid,
            ssl_error=self.ssl_error,
            tls_version=self.tls_version,
            http_to_https=self.http_to_https,
            www_consistency=self.www_consistency,
            homepage_status_code=self.homepage_status_code,
            homepage_ttfb=self.homepage_ttfb,
            homepage_headers=dict(self.homepage_headers),
            has_llms_txt=self.has_llms_txt,
            llms_txt_content=self.llms_txt_content,
        )

        # Run cross-page analysis
        analyzer = SiteAnalyzer(
            result,
            self.internal_link_targets,
            self.sitemap_urls,
            self.page_outgoing_links,
            unresolved_internal_status,
            external_status_cache,
            self.raw_url_variants,
            canonical_status_cache,
        )
        analyzer.analyze()

        # Scoring
        scorer = ReportGenerator(result)
        scorer.calculate_scores()
        scorer.generate_recommendations()

        try:
            self.session.close()
        except Exception:
            pass

        return result

    def stop(self):
        self._stop_flag = True

    def _fetch_url_status(self, url: str, timeout: int = 10) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç HTTP-—Å—Ç–∞—Ç—É—Å URL (HEAD -> GET)."""
        try:
            head = self.session.head(
                url,
                timeout=timeout,
                allow_redirects=True,
                verify=True,
                headers=self._request_headers(),
            )
            if head.status_code >= 400 or head.status_code == 405:
                get_resp = self.session.get(
                    url,
                    timeout=timeout,
                    allow_redirects=True,
                    verify=True,
                    headers=self._request_headers(),
                )
                return get_resp.status_code
            return head.status_code
        except requests.exceptions.SSLError:
            try:
                get_resp = self.session.get(
                    url,
                    timeout=timeout,
                    allow_redirects=True,
                    verify=False,
                    headers=self._request_headers(),
                )
                return get_resp.status_code
            except Exception:
                return 0
        except Exception:
            return 0

    def _resolve_uncrawled_internal_statuses(self) -> Dict[str, int]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å—ã –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö URL, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏—Å—å –≤ —Å—Å—ã–ª–∫–∞—Ö, –Ω–æ –Ω–µ –±—ã–ª–∏ –ø—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω—ã."""
        unresolved = set()
        crawled_norm = {self.normalize_url(u) for u in self.results.keys()}
        for links in self.page_outgoing_links.values():
            for target in links["internal"]:
                if target not in crawled_norm:
                    unresolved.add(target)

        status_map: Dict[str, int] = {}
        if not unresolved:
            return status_map

        for url in list(unresolved)[:MAX_UNCRAWLED_STATUS_CHECKS]:
            status_map[url] = self._fetch_url_status(url, timeout=8)
        return status_map

    def _resolve_canonical_statuses(self) -> Dict[str, int]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–¥—ã –æ—Ç–≤–µ—Ç–∞ canonical-URL –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö."""
        canonical_targets = set()
        for page in self.results.values():
            if page.canonical:
                absolute = page.canonical if page.canonical.startswith("http") else urljoin(page.url, page.canonical)
                absolute = self.normalize_url(absolute)
                page.canonical_absolute = absolute
                canonical_targets.add(absolute)

        if not canonical_targets:
            return {}

        status_cache: Dict[str, int] = {}
        for target in canonical_targets:
            if target in self.results:
                status_cache[target] = self.results[target].status_code
            else:
                status_cache[target] = self._fetch_url_status(target, timeout=8)

        for page in self.results.values():
            if page.canonical_absolute:
                page.canonical_target_status = status_cache.get(page.canonical_absolute, 0)
                if page.canonical_target_status >= 400:
                    page.issues.append(PageIssue(
                        "critical",
                        "technical",
                        "canonical_target_error",
                        "Canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ URL —Å –æ—à–∏–±–∫–æ–π 4xx/5xx",
                        str(page.canonical_target_status),
                        "200",
                    ))
        return status_cache

    def _check_external_links(self, progress_callback=None) -> Dict[str, int]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ (–µ—Å–ª–∏ –æ–ø—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞)."""
        external_urls = list(self.external_links_found)[:MAX_EXTERNAL_CHECKS]
        status_map: Dict[str, int] = {}
        if not external_urls:
            return status_map

        if progress_callback:
            progress_callback({"type": "pre_check", "message": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫..."})

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self._fetch_url_status, url, 8): url for url in external_urls}
            for future in as_completed(futures):
                status_map[futures[future]] = future.result()
        return status_map

    def _fetch_and_analyze(self, url: str, depth: int) -> Tuple[Optional[PageResult], List[str], List[str]]:
        """Fetch a single page and run per-page analysis."""
        result = PageResult(url=url, crawl_depth=depth)
        discovered_internal: List[str] = []
        discovered_external: List[str] = []

        try:
            resp = self.session.get(
                url, timeout=15, allow_redirects=True, verify=True,
                headers=self._request_headers(),
                stream=True,
            )
        except requests.exceptions.SSLError:
            try:
                resp = self.session.get(
                    url,
                    timeout=15,
                    allow_redirects=True,
                    verify=False,
                    headers=self._request_headers(),
                    stream=True,
                )
            except Exception as e:
                result.error_message = f"SSL + fallback error: {str(e)[:100]}"
                result.issues.append(PageIssue("critical", "security", "ssl_error",
                    "–û—à–∏–±–∫–∞ SSL-—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞", str(e)[:100], "–í–∞–ª–∏–¥–Ω—ã–π —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç"))
                return result, [], []
        except requests.exceptions.Timeout:
            result.error_message = "Timeout"
            result.issues.append(PageIssue("critical", "technical", "timeout",
                "–¢–∞–π–º–∞—É—Ç: —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ 15 —Å–µ–∫—É–Ω–¥"))
            return result, [], []
        except requests.exceptions.ConnectionError as e:
            result.error_message = str(e)[:100]
            result.issues.append(PageIssue("critical", "technical", "connection_error",
                "–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"))
            return result, [], []
        except requests.exceptions.InvalidURL as e:
            result.error_message = str(e)[:150]
            result.issues.append(PageIssue("critical", "technical", "invalid_url",
                "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL", str(e)[:100], "–≤–∞–ª–∏–¥–Ω—ã–π URL"))
            return result, [], []
        except requests.exceptions.TooManyRedirects:
            result.error_message = "Too many redirects"
            result.issues.append(PageIssue("critical", "technical", "redirect_loop",
                "–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤", ">5 —Ö–æ–ø–æ–≤", "1-2 —Ö–æ–ø–∞"))
            return result, [], []
        except Exception as e:
            result.error_message = str(e)[:200]
            return result, [], []

        # Basic response info
        result.url = self.normalize_url(resp.url or url)
        result.status_code = resp.status_code
        result.ttfb = resp.elapsed.total_seconds()
        result.content_type = resp.headers.get("Content-Type", "")
        result.response_headers = {k: v for k, v in resp.headers.items()}
        if "text/html" not in result.content_type.lower():
            result.issues.append(PageIssue(
                "warning",
                "technical",
                "non_html_content",
                "–ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ text/html",
                result.content_type or "unknown",
                "text/html",
            ))
            try:
                resp.close()
            except Exception:
                pass
            return result, [], []

        raw_chunks: List[bytes] = []
        loaded_size = 0
        too_large = False
        content_len_header = resp.headers.get("Content-Length", "").strip()
        declared_size = int(content_len_header) if content_len_header.isdigit() else 0
        # Always load at least LINK_EXTRACT_SIZE to discover links for BFS,
        # even if page exceeds MAX_HTML_SIZE.
        LINK_EXTRACT_SIZE = MAX_HTML_SIZE  # load up to limit regardless
        try:
            for chunk in resp.iter_content(chunk_size=65536):
                if not chunk:
                    continue
                loaded_size += len(chunk)
                raw_chunks.append(chunk)
                if loaded_size > MAX_HTML_SIZE:
                    too_large = True
                    break
        except Exception:
            pass

        raw_bytes = b"".join(raw_chunks)
        result.content_length = loaded_size if loaded_size > 0 else declared_size

        try:
            resp.close()
        except Exception:
            pass

        if result.content_length == 0 and len(raw_bytes) == 0:
            result.issues.append(PageIssue(
                "critical",
                "technical",
                "empty_response",
                "–°–∞–π—Ç –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç",
                "0 –±–∞–π—Ç",
                ">0 –±–∞–π—Ç",
            ))
            return result, [], []
        if too_large:
            page_kb = max(loaded_size, declared_size) // 1024
            result.issues.append(PageIssue(
                "warning",
                "technical",
                "large_page",
                f"–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–∞: {page_kb}KB",
                f"{page_kb}KB",
                f"<{MAX_HTML_SIZE // 1024}KB",
            ))
            # Do NOT return early ‚Äî continue to parse links from loaded portion

        try:
            encoding = resp.encoding or "utf-8"
            html_text = raw_bytes.decode(encoding, errors="replace")
        except (LookupError, UnicodeDecodeError):
            html_text = raw_bytes.decode("utf-8", errors="replace")
        result.raw_html = html_text[:MAX_STORED_HTML_CHARS]
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä, —á—Ç–æ–±—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞–ª —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞.
        resp._content = raw_bytes

        if result.status_code in (403, 429):
            result.issues.append(PageIssue(
                "warning",
                "technical",
                "blocked_by_server",
                "–°–∞–π—Ç –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫—Ä–∞—É–ª–∏–Ω–≥ (403/429). –£–≤–µ–ª–∏—á—å—Ç–µ –∑–∞–¥–µ—Ä–∂–∫—É –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø.",
                str(result.status_code),
                "200",
            ))
        body_preview = html_text[:2500].lower()
        if "cloudflare" in body_preview and ("attention required" in body_preview or "cf-challenge" in body_preview):
            result.issues.append(PageIssue(
                "warning",
                "technical",
                "cloudflare_protection",
                "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ Cloudflare. –ß–∞—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.",
            ))

        # Redirect chain
        if resp.history:
            result.redirect_chain = [r.url for r in resp.history] + [resp.url]
            if len(resp.history) > 0:
                result.redirect_type = resp.history[0].status_code

        # HSTS
        result.has_hsts = "strict-transport-security" in resp.headers

        # X-Robots-Tag
        x_robots = resp.headers.get("X-Robots-Tag", "")
        if x_robots:
            result.x_robots_tag = x_robots
            lower_xrobots = x_robots.lower()
            if "noindex" in lower_xrobots:
                result.is_indexable = False
                result.issues.append(PageIssue(
                    "info",
                    "technical",
                    "xrobots_noindex",
                    "X-Robots-Tag —Å–æ–¥–µ—Ä–∂–∏—Ç noindex",
                ))
            if "nofollow" in lower_xrobots:
                result.issues.append(PageIssue(
                    "info",
                    "technical",
                    "xrobots_nofollow",
                    "X-Robots-Tag —Å–æ–¥–µ—Ä–∂–∏—Ç nofollow",
                ))

        # Parse HTML
        try:
            soup = BeautifulSoup(raw_bytes, "lxml")
        except Exception:
            soup = BeautifulSoup(raw_bytes, "html.parser")

        # Run all per-page checks
        page_analyzer = PageAnalyzer(
            result.url,
            raw_bytes,
            soup,
            result,
            self.domain,
            self.base_origin,
            self.session,
            self._request_headers,
            self.resource_info_cache,
            self.resource_cache_lock,
        )
        page_analyzer.analyze_all()

        # Extract links for crawler
        discovered_internal, discovered_external = page_analyzer.extract_links()
        result.internal_link_urls = [self.normalize_url(x) for x in discovered_internal][:300]
        result.external_link_urls = discovered_external[:300]

        return result, discovered_internal, discovered_external


# === PAGE ANALYZER ===

class PageAnalyzer:
    """Performs all per-page SEO checks."""

    def __init__(self, url: str, raw_bytes: bytes, soup: BeautifulSoup,
                 result: PageResult, domain: str, base_origin: str,
                 session: requests.Session, headers_factory,
                 resource_info_cache: Dict[str, Tuple[int, int]],
                 resource_cache_lock: threading.Lock):
        self.url = url
        self.raw_bytes = raw_bytes  # raw HTML bytes ‚Äî no dependency on response object
        self.soup = soup
        self.r = result
        self.domain = domain
        self.base_origin = base_origin
        self.session = session
        self.headers_factory = headers_factory
        self.resource_info_cache = resource_info_cache
        self.resource_cache_lock = resource_cache_lock

    def add_issue(self, severity: str, category: str, code: str, message: str,
                  current: str = "", expected: str = ""):
        self.r.issues.append(PageIssue(severity, category, code, message, current, expected))

    def analyze_all(self):
        self._check_url_structure()
        self._check_status()
        self._check_ttfb()
        self._check_redirects()
        self._check_title()
        self._check_description()
        self._check_meta_robots()
        self._check_headings()
        self._check_content()
        self._check_canonical()
        self._check_images()
        self._check_schema()
        self._check_og()
        self._check_twitter()
        self._check_viewport()
        self._check_charset()
        self._check_mixed_content()
        self._check_lang()
        self._check_hreflang()
        self._detect_framework()
        self._detect_js_render_risk()

    def _check_url_structure(self):
        parsed = urlparse(self.url)
        if len(self.url) > 1024:
            self.add_issue(
                "warning",
                "technical",
                "long_url",
                "URL —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–º–æ–∂–µ—Ç –º–µ—à–∞—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)",
                str(len(self.url)),
                "<=1024",
            )
        query_params = parse_qs(parsed.query, keep_blank_values=True)
        if len(query_params) > 6:
            self.add_issue(
                "info",
                "technical",
                "too_many_query_params",
                "–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ URL-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Ä–∏—Å–∫ –¥—É–±–ª–µ–π)",
                str(len(query_params)),
                "<=6",
            )

    def _fetch_resource_info(self, resource_url: str) -> Tuple[int, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–¥ –æ—Ç–≤–µ—Ç–∞ –∏ —Ä–∞–∑–º–µ—Ä —Ä–µ—Å—É—Ä—Å–∞ (–±–∞–π—Ç—ã)."""
        cache_key = resource_url.split("#", 1)[0]
        with self.resource_cache_lock:
            cached = self.resource_info_cache.get(cache_key)
        if cached:
            return cached

        status_size: Tuple[int, int] = (0, 0)
        try:
            head = self.session.head(
                cache_key,
                timeout=RESOURCE_CHECK_TIMEOUT,
                allow_redirects=True,
                verify=True,
                headers=self.headers_factory(),
            )
            size = int(head.headers.get("Content-Length", "0") or "0")
            if head.status_code == 405:
                with self.session.get(
                    cache_key,
                    timeout=RESOURCE_CHECK_TIMEOUT,
                    allow_redirects=True,
                    verify=True,
                    headers=self.headers_factory(),
                    stream=True,
                ) as get_resp:
                    size = int(get_resp.headers.get("Content-Length", "0") or "0")
                    status_size = (get_resp.status_code, size)
            else:
                status_size = (head.status_code, size)
        except requests.exceptions.SSLError:
            try:
                with self.session.get(
                    cache_key,
                    timeout=RESOURCE_CHECK_TIMEOUT,
                    allow_redirects=True,
                    verify=False,
                    headers=self.headers_factory(),
                    stream=True,
                ) as get_resp:
                    size = int(get_resp.headers.get("Content-Length", "0") or "0")
                    status_size = (get_resp.status_code, size)
            except Exception:
                status_size = (0, 0)
        except Exception:
            status_size = (0, 0)

        with self.resource_cache_lock:
            self.resource_info_cache[cache_key] = status_size
        return status_size

    # --- Individual checks ---

    def _check_status(self):
        code = self.r.status_code
        if code >= 500:
            self.add_issue("critical", "technical", "server_error",
                f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {code}", str(code), "200")
        elif code >= 400:
            self.add_issue("critical", "technical", "client_error",
                f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {code}", str(code), "200")
        elif 300 <= code < 400:
            self.add_issue("warning", "technical", "redirect",
                f"–†–µ–¥–∏—Ä–µ–∫—Ç: {code}", str(code), "200")

    def _check_ttfb(self):
        ttfb = self.r.ttfb
        if ttfb > TTFB_CRITICAL:
            self.add_issue("critical", "technical", "slow_ttfb",
                f"–û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {ttfb:.2f}—Å", f"{ttfb:.2f}—Å", "<1.5—Å")
        elif ttfb > TTFB_WARNING:
            self.add_issue("warning", "technical", "slow_ttfb",
                f"–ú–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {ttfb:.2f}—Å", f"{ttfb:.2f}—Å", "<1.5—Å")

    def _check_redirects(self):
        chain = self.r.redirect_chain
        if len(chain) > 2:
            severity = "critical" if len(chain) > 3 else "warning"
            self.add_issue(severity, "technical", "redirect_chain",
                f"–¶–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤: {len(chain)} —Ö–æ–ø–æ–≤", f"{len(chain)} —Ö–æ–ø–æ–≤", "1-2 —Ö–æ–ø–∞")
        if len(chain) > MAX_REDIRECTS:
            self.add_issue("critical", "technical", "redirect_loop",
                "–ü–æ—Ö–æ–∂–µ –Ω–∞ —Ü–∏–∫–ª —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤", f"{len(chain)} —Ö–æ–ø–æ–≤", "1-2 —Ö–æ–ø–∞")

    def _check_title(self):
        title_tag = self.soup.find("title")
        if not title_tag:
            self.add_issue("critical", "content", "missing_title", "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ Title")
            return

        title_text = title_tag.get_text(strip=True)
        self.r.title = title_text
        self.r.title_length = len(title_text)

        if not title_text:
            self.add_issue("critical", "content", "empty_title",
                "–ü—É—Å—Ç–æ–π —Ç–µ–≥ Title", "–ø—É—Å—Ç–æ", "30-60 —Å–∏–º–≤–æ–ª–æ–≤")
        elif len(title_text) < TITLE_MIN:
            self.add_issue("warning", "content", "short_title",
                f"Title —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {len(title_text)} —Å–∏–º–≤.",
                f"{len(title_text)} —Å–∏–º–≤.", f">{TITLE_MIN} —Å–∏–º–≤.")
        elif len(title_text) > TITLE_MAX:
            self.add_issue("warning", "content", "long_title",
                f"Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π: {len(title_text)} —Å–∏–º–≤.",
                f"{len(title_text)} —Å–∏–º–≤.", f"<{TITLE_MAX} —Å–∏–º–≤.")

    def _check_description(self):
        desc_tag = self.soup.find("meta", attrs={"name": re.compile(r"^description$", re.I)})
        if not desc_tag:
            self.add_issue("warning", "content", "missing_description",
                "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç Meta Description")
            return

        desc_text = desc_tag.get("content", "").strip()
        self.r.description = desc_text
        self.r.description_length = len(desc_text)

        if not desc_text:
            self.add_issue("warning", "content", "empty_description",
                "–ü—É—Å—Ç–æ–π Meta Description", "–ø—É—Å—Ç–æ", "70-160 —Å–∏–º–≤–æ–ª–æ–≤")
        elif len(desc_text) < DESC_MIN:
            self.add_issue("info", "content", "short_description",
                f"Description –∫–æ—Ä–æ—Ç–∫–∏–π: {len(desc_text)} —Å–∏–º–≤.",
                f"{len(desc_text)} —Å–∏–º–≤.", f">{DESC_MIN} —Å–∏–º–≤.")
        elif len(desc_text) > DESC_MAX:
            self.add_issue("warning", "content", "long_description",
                f"Description –¥–ª–∏–Ω–Ω—ã–π: {len(desc_text)} —Å–∏–º–≤.",
                f"{len(desc_text)} —Å–∏–º–≤.", f"<{DESC_MAX} —Å–∏–º–≤.")

    def _check_meta_robots(self):
        meta_robots = self.soup.find("meta", attrs={"name": re.compile(r"^robots$", re.I)})
        if meta_robots:
            content = meta_robots.get("content", "").lower()
            self.r.has_meta_robots = True
            self.r.meta_robots = content
            if "noindex" in content:
                self.r.is_indexable = False
                self.add_issue("info", "technical", "noindex",
                    "–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–∫—Ä—ã—Ç–∞ –æ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (meta robots noindex)")
            if "nofollow" in content:
                self.add_issue("info", "technical", "nofollow",
                    "–°—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ –ø–µ—Ä–µ–¥–∞—é—Ç –≤–µ—Å (meta robots nofollow)")
            for directive in ("noarchive", "nosnippet"):
                if directive in content:
                    self.add_issue("info", "technical", f"meta_{directive}",
                        f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –¥–∏—Ä–µ–∫—Ç–∏–≤–Ω—ã–π —Ç–µ–≥ robots: {directive}")

    def _check_headings(self):
        # H1
        h1_tags = self.soup.find_all("h1")
        self.r.h1_list = [h.get_text(strip=True) for h in h1_tags]
        self.r.h1_count = len(h1_tags)
        empty_h1_count = sum(1 for h in h1_tags if not h.get_text(strip=True))

        if not h1_tags:
            self.add_issue("critical", "content", "missing_h1",
                "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ H1", "0 —à—Ç.", "1 —à—Ç.")
        elif empty_h1_count > 0:
            self.add_issue("critical", "content", "empty_h1",
                f"–ü—É—Å—Ç–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ H1: {empty_h1_count} —à—Ç.", str(empty_h1_count), "0")
        if len(h1_tags) > 1:
            self.add_issue("warning", "content", "multiple_h1",
                f"–ù–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ H1: {len(h1_tags)} —à—Ç.",
                f"{len(h1_tags)} —à—Ç.", "1 —à—Ç.")
        elif len(h1_tags) == 1:
            h1_text = h1_tags[0].get_text(strip=True)
            if len(h1_text) > H1_MAX_LEN:
                self.add_issue("info", "content", "long_h1",
                    f"H1 —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π: {len(h1_text)} —Å–∏–º–≤.",
                    f"{len(h1_text)} —Å–∏–º–≤.", f"<{H1_MAX_LEN} —Å–∏–º–≤.")

        # H1 vs Title
        if self.r.title and self.r.h1_list:
            if self.r.title.strip().lower() == self.r.h1_list[0].strip().lower():
                self.add_issue("info", "content", "h1_equals_title",
                    "H1 –∏–¥–µ–Ω—Ç–∏—á–µ–Ω Title (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞–∑–ª–∏—á–∞—Ç—å)")

        # Full heading hierarchy in DOM order
        headings = []
        for h in self.soup.find_all(re.compile(r"^h[1-6]$", re.I)):
            level = int(h.name[1])
            headings.append({"level": level, "text": h.get_text(strip=True)[:100]})
        self.r.headings = headings

        # Check hierarchy
        if headings:
            prev_level = 0
            hierarchy_broken = False
            for h in headings:
                if h["level"] > prev_level + 1 and prev_level > 0:
                    hierarchy_broken = True
                    break
                prev_level = h["level"]
            if hierarchy_broken:
                self.add_issue("info", "content", "heading_hierarchy",
                    "–ù–∞—Ä—É—à–µ–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–ø—Ä–æ–ø—É—â–µ–Ω —É—Ä–æ–≤–µ–Ω—å)")

    def _check_content(self):
        try:
            clean_soup = BeautifulSoup(self.raw_bytes, "lxml")
        except Exception:
            clean_soup = BeautifulSoup(self.raw_bytes, "html.parser")

        # Remove script/style/comment content for word count
        for elem in clean_soup(["script", "style", "noscript"]):
            elem.decompose()
        # Remove comments
        for comment in clean_soup.find_all(string=lambda t: isinstance(t, Comment)):
            comment.extract()

        text = clean_soup.get_text(separator=" ", strip=True)
        words = text.split()
        self.r.word_count = len(words)
        self.r.content_text = re.sub(r"\s+", " ", text).strip()[:MAX_CONTENT_TEXT_CHARS]

        # Content hash
        clean_text = re.sub(r"\s+", " ", text.lower().strip())
        self.r.content_hash = hashlib.md5(clean_text.encode("utf-8")).hexdigest()

        # Text to HTML ratio
        html_len = self.r.content_length
        text_len = len(text.encode("utf-8"))
        self.r.text_html_ratio = (text_len / max(1, html_len)) * 100

        # Thin content checks ‚Äî skip non-content pages to reduce false positives
        path_lower = urlparse(self.url).path.lower()
        is_non_content_page = any(seg in path_lower for seg in [
            "/contact", "/login", "/register", "/signup", "/auth",
            "/cart", "/checkout", "/account", "/search", "/404",
            "/privacy", "/terms", "/legal", "/cookie", "/gdpr",
            "/tag/", "/category/", "/feed", "/rss",
        ])
        is_blog_like = "/blog" in path_lower or "/news" in path_lower

        # Pages with error status codes shouldn't be flagged for thin content
        is_error_page = self.r.status_code >= 400

        if not is_non_content_page and not is_error_page:
            if self.r.word_count < VERY_THIN_CONTENT_THRESHOLD:
                self.add_issue("warning", "content", "very_thin_content",
                    f"–û—á–µ–Ω—å –º–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {self.r.word_count} —Å–ª–æ–≤",
                    f"{self.r.word_count} —Å–ª–æ–≤", f">{THIN_CONTENT_THRESHOLD} —Å–ª–æ–≤")
            elif self.r.word_count < THIN_CONTENT_THRESHOLD and not is_blog_like:
                self.add_issue("warning", "content", "thin_content",
                    f"–ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {self.r.word_count} —Å–ª–æ–≤",
                    f"{self.r.word_count} —Å–ª–æ–≤", f">{THIN_CONTENT_THRESHOLD} —Å–ª–æ–≤")

        if self.r.text_html_ratio < TEXT_HTML_RATIO_LOW:
            self.add_issue("info", "content", "low_text_ratio",
                f"–ù–∏–∑–∫–∏–π —Ç–µ–∫—Å—Ç/HTML: {self.r.text_html_ratio:.1f}%",
                f"{self.r.text_html_ratio:.1f}%", f">{TEXT_HTML_RATIO_LOW}%")

    def _check_canonical(self):
        canonical_tags = self.soup.find_all("link", attrs={"rel": "canonical"})

        if not canonical_tags:
            self.r.canonical_status = "missing"
            self.add_issue("warning", "technical", "missing_canonical",
                "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ Canonical")
            return

        if len(canonical_tags) > 1:
            self.add_issue("critical", "technical", "multiple_canonical",
                f"–ù–∞–π–¥–µ–Ω–æ {len(canonical_tags)} —Ç–µ–≥–æ–≤ Canonical",
                f"{len(canonical_tags)} —à—Ç.", "1 —à—Ç.")

        canonical_href = canonical_tags[0].get("href", "").strip()
        self.r.canonical = canonical_href

        if not canonical_href:
            self.r.canonical_status = "error"
            self.add_issue("warning", "technical", "empty_canonical",
                "–ü—É—Å—Ç–æ–π —Ç–µ–≥ Canonical")
            return

        # Check relative
        if not canonical_href.startswith("http"):
            self.add_issue("warning", "technical", "relative_canonical",
                "Canonical —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL",
                canonical_href, "https://...")
        absolute_canonical = canonical_href if canonical_href.startswith("http") else urljoin(self.url, canonical_href)
        self.r.canonical_absolute = absolute_canonical

        # Check protocol mismatch
        parsed_page = urlparse(self.url)
        parsed_can = urlparse(absolute_canonical)
        if parsed_page.scheme == "https" and parsed_can.scheme == "http":
            self.add_issue("critical", "technical", "canonical_http",
                "Canonical —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ HTTP (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ HTTPS)",
                canonical_href, f"https://{parsed_can.netloc}{parsed_can.path}")

        # Check self-referencing
        # Use proper normalization for canonical comparison to avoid false positives
        # on www/non-www, trailing slash, scheme differences
        def _norm_for_cmp(u: str) -> str:
            p = urlparse(u)
            return urlunparse((p.scheme.lower(), p.netloc.lower().replace("www.", ""),
                               p.path.rstrip("/") or "/", "", "", ""))
        norm_page = _norm_for_cmp(self.url)
        norm_can = _norm_for_cmp(absolute_canonical)
        if norm_can and norm_can != norm_page:
            self.r.canonical_status = "other"
            # INFO not WARNING: many pages intentionally point canonical elsewhere
            # (paginated, filtered, language variants)
            self.add_issue("info", "technical", "canonical_not_self",
                "Canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–π URL (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —ç—Ç–æ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ)",
                canonical_href, self.url)
        else:
            self.r.canonical_status = "ok"

    def _check_images(self):
        images = self.soup.find_all("img")
        self.r.images_total = len(images)
        missing_alt = 0
        empty_alt_non_decorative = 0
        missing_dims = 0
        broken_images = 0
        large_images = 0
        checked = 0

        for img in images:
            alt = img.get("alt")
            src = (img.get("src") or "").strip()

            if alt is None:
                missing_alt += 1
            elif alt.strip() == "":
                is_decorative = any(marker in src.lower() for marker in ["icon", "logo", "sprite", "spacer"])
                if not is_decorative:
                    empty_alt_non_decorative += 1

            if not img.get("width") and not img.get("height"):
                # Check for style attribute
                style = img.get("style", "")
                if "width" not in style and "height" not in style:
                    missing_dims += 1

            if src and checked < MAX_IMAGE_RESOURCE_CHECKS:
                if src.startswith(("data:", "blob:", "javascript:")):
                    continue
                abs_img = urljoin(self.url, src)
                if not abs_img.startswith(("http://", "https://")):
                    continue
                # Respect global cap to avoid excessive network calls
                with self.resource_cache_lock:
                    total_checked = len(self.resource_info_cache)
                if total_checked >= MAX_TOTAL_RESOURCE_CHECKS:
                    break
                checked += 1
                status, content_len = self._fetch_resource_info(abs_img)
                if status >= 400:
                    broken_images += 1
                if content_len > LARGE_IMAGE_BYTES:
                    large_images += 1

        self.r.images_missing_alt = missing_alt
        self.r.images_empty_alt = empty_alt_non_decorative
        self.r.images_missing_dimensions = missing_dims
        self.r.images_broken = broken_images
        self.r.images_large = large_images

        if missing_alt > 0:
            self.add_issue("warning", "images", "missing_alt",
                f"{missing_alt} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ ALT",
                f"{missing_alt} —à—Ç.", "0 —à—Ç.")
        if empty_alt_non_decorative > 0:
            self.add_issue("info", "images", "empty_alt_non_decorative",
                f"{empty_alt_non_decorative} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—É—Å—Ç—ã–º ALT",
                f"{empty_alt_non_decorative} —à—Ç.", "ALT —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º")

        if missing_dims > 0:
            self.add_issue("warning", "images", "missing_dimensions",
                f"{missing_dims} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ width/height (CLS)",
                f"{missing_dims} —à—Ç.", "0 —à—Ç.")
        if broken_images > 0:
            self.add_issue("warning", "images", "broken_images",
                f"{broken_images} –±–∏—Ç—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (4xx/5xx)",
                f"{broken_images} —à—Ç.", "0 —à—Ç.")
        if large_images > 0:
            self.add_issue("info", "images", "large_images",
                f"{large_images} —Ç—è–∂—ë–ª—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (>500KB)",
                f"{large_images} —à—Ç.", "0 —à—Ç.")

    def _check_schema(self):
        schema_soup = self.soup

        ld_scripts = schema_soup.find_all("script", type="application/ld+json")
        if ld_scripts:
            self.r.has_schema = True
            for script in ld_scripts:
                try:
                    payload = script.string or script.get_text() or ""
                    data = json.loads(payload)
                    if isinstance(data, dict):
                        t = data.get("@type", "")
                        if t:
                            self.r.schema_types.append(t)
                    elif isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict):
                                t = item.get("@type", "")
                                if t:
                                    self.r.schema_types.append(t)
                except (json.JSONDecodeError, TypeError):
                    self.add_issue("warning", "structured_data", "invalid_json_ld",
                        "–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON-LD (–æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞)")
        # Microdata
        if schema_soup.find(attrs={"itemscope": True}):
            self.r.has_microdata = True

    def _check_og(self):
        og_title = self.soup.find("meta", property="og:title")
        og_desc = self.soup.find("meta", property="og:description")
        og_image = self.soup.find("meta", property="og:image")

        if og_title:
            self.r.has_og = True
            self.r.og_tags["title"] = og_title.get("content", "")
        if og_desc:
            self.r.og_tags["description"] = og_desc.get("content", "")
        if og_image:
            self.r.og_tags["image"] = og_image.get("content", "")

        if not og_title:
            self.add_issue("info", "structured_data", "missing_og_title", "–ù–µ—Ç og:title")
        if not og_desc:
            self.add_issue("info", "structured_data", "missing_og_desc", "–ù–µ—Ç og:description")
        if not og_image:
            self.add_issue("info", "structured_data", "missing_og_image", "–ù–µ—Ç og:image")

    def _check_twitter(self):
        tc = self.soup.find("meta", attrs={"name": "twitter:card"})
        if tc:
            self.r.has_twitter_card = True
        else:
            self.add_issue("info", "structured_data", "missing_twitter",
                "–ù–µ—Ç Twitter Card –º–µ—Ç–∞-—Ç–µ–≥–∞")

    def _check_viewport(self):
        vp = self.soup.find("meta", attrs={"name": "viewport"})
        if vp:
            self.r.has_viewport = True
        else:
            self.add_issue("critical", "technical", "missing_viewport",
                "–ù–µ—Ç –º–µ—Ç–∞-—Ç–µ–≥–∞ Viewport (–Ω–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö)")

    def _check_charset(self):
        charset_meta = self.soup.find("meta", charset=True)
        if charset_meta:
            self.r.has_charset = True
            self.r.charset_value = charset_meta.get("charset", "")
            if self.r.charset_value.lower() not in ("utf-8", "utf8"):
                self.add_issue("warning", "technical", "wrong_charset",
                    f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞ –Ω–µ UTF-8: {self.r.charset_value}",
                    self.r.charset_value, "UTF-8")
        else:
            # Check Content-Type header for charset
            ct = self.r.content_type
            if "charset" in ct.lower():
                self.r.has_charset = True
            else:
                # Check meta http-equiv
                http_equiv = self.soup.find("meta", attrs={"http-equiv": re.compile(r"content-type", re.I)})
                if http_equiv:
                    self.r.has_charset = True
                else:
                    self.add_issue("warning", "technical", "missing_charset",
                        "–ù–µ —É–∫–∞–∑–∞–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã")

    def _check_mixed_content(self):
        """Check for HTTP resources on HTTPS page."""
        if urlparse(self.url).scheme != "https":
            return

        mc_soup = self.soup

        mixed = False
        for tag, attr in [("img", "src"), ("script", "src"), ("link", "href"),
                          ("iframe", "src"), ("source", "src"), ("video", "src"),
                          ("audio", "src")]:
            for elem in mc_soup.find_all(tag, attrs={attr: True}):
                src = elem.get(attr, "")
                if src.startswith("http://") and not src.startswith("http://localhost"):
                    mixed = True
                    break
            if mixed:
                break

        if mixed:
            self.r.has_mixed_content = True
            self.add_issue("warning", "security", "mixed_content",
                "–°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: HTTPS-—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç HTTP-—Ä–µ—Å—É—Ä—Å—ã")

    def _check_lang(self):
        html_tag = self.soup.find("html")
        if html_tag and html_tag.get("lang"):
            self.r.has_lang = True
            self.r.lang_value = html_tag.get("lang", "")
        else:
            self.add_issue("info", "technical", "missing_lang",
                "–ù–µ —É–∫–∞–∑–∞–Ω –∞—Ç—Ä–∏–±—É—Ç lang —É —Ç–µ–≥–∞ <html>")

    def _check_hreflang(self):
        hreflangs = self.soup.find_all("link", attrs={"hreflang": True})
        if hreflangs:
            self.r.has_hreflang = True
            for hl in hreflangs:
                self.r.hreflang_tags.append({
                    "lang": hl.get("hreflang", ""),
                    "href": hl.get("href", ""),
                })
            return

        # Stricter multilingual detection to reduce false positives
        # Only detect if path starts with /xx/ where xx is a 2-letter ISO code
        import re as _re
        candidate_langs = set()
        path = urlparse(self.url).path.lower()
        # Check if current page path starts with /xx/
        lang_prefix_match = _re.match(r"^/([a-z]{2})(?:/|$)", path)
        if lang_prefix_match:
            candidate_langs.add(lang_prefix_match.group(1))

        # Check internal links for /xx/ prefixes (limit scan to 50 links)
        for i, a in enumerate(self.soup.find_all("a", href=True)):
            if i >= 50:
                break
            href = a.get("href", "").strip()
            if not href or href.startswith(("#", "javascript:", "mailto:")):
                continue
            href_path = urlparse(href).path.lower()
            m = _re.match(r"^/([a-z]{2})(?:/|$)", href_path)
            if m:
                candidate_langs.add(m.group(1))
            if len(candidate_langs) >= 2:
                break

        # Only flag if we found 2+ distinct language-like prefixes
        # AND they are actual ISO 639-1 codes (not random 2-letter path segments)
        iso_codes = {"ru", "en", "de", "fr", "es", "it", "pt", "tr", "pl", "uk",
                     "zh", "ja", "ar", "ko", "nl", "sv", "no", "da", "fi", "cs", "ro", "hu"}
        real_langs = candidate_langs & iso_codes
        if len(real_langs) >= 2:
            self.add_issue(
                "info",
                "technical",
                "missing_hreflang",
                "–ü–æ—Ö–æ–∂–µ –Ω–∞ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π —Å–∞–π—Ç, –Ω–æ hreflang –Ω–µ –Ω–∞–π–¥–µ–Ω",
            )

    def _detect_framework(self):
        html_str = (self.r.raw_html or "")[:5000].lower()
        detected = []
        for name, patterns in JS_FRAMEWORKS.items():
            for pat in patterns:
                if pat.lower() in html_str:
                    detected.append(name)
                    break
        if detected:
            self.r.detected_framework = detected[0]

    def _detect_js_render_risk(self):
        script_count = len(self.soup.find_all("script"))
        if script_count > 15 and self.r.word_count < 80:
            self.r.js_render_warning = True
            self.add_issue(
                "info",
                "technical",
                "js_rendering_risk",
                "–ü–æ—Ö–æ–∂–µ, –∫–æ–Ω—Ç–µ–Ω—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ JavaScript. –ß–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –æ–±—ã—á–Ω–æ–º—É –∫—Ä–∞—É–ª–µ—Ä—É.",
            )

    def extract_links(self) -> Tuple[List[str], List[str]]:
        """Extract all links from page for crawler."""
        link_soup = self.soup

        internal_links: List[str] = []
        external_links: List[str] = []
        internal = 0
        external = 0
        nofollow_internal = 0
        empty_href = 0

        for a in link_soup.find_all("a"):
            href = (a.get("href") or "").strip()
            if not href or href == "#":
                empty_href += 1
                continue
            if href.startswith(("javascript:", "mailto:", "tel:")):
                continue

            abs_url = urljoin(self.url, href)
            parsed = urlparse(abs_url)

            if parsed.scheme not in ("http", "https"):
                continue

            is_int = (
                parsed.netloc == self.domain
                or parsed.netloc == ""
                or parsed.netloc.replace("www.", "") == self.domain.replace("www.", "")
            )
            if is_int:
                internal += 1
                internal_links.append(abs_url)
                # Check nofollow on internal
                rel = a.get("rel", [])
                if isinstance(rel, list):
                    if "nofollow" in rel:
                        nofollow_internal += 1
                elif "nofollow" in str(rel):
                    nofollow_internal += 1
            else:
                external += 1
                external_links.append(abs_url)

        self.r.internal_links = internal
        self.r.external_links = external
        self.r.empty_href_links = empty_href
        self.r.internal_links_nofollow = nofollow_internal

        if nofollow_internal > 0:
            self.add_issue("warning", "links", "nofollow_internal",
                f"{nofollow_internal} –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ —Å nofollow",
                f"{nofollow_internal} —à—Ç.", "0 —à—Ç.")
        if empty_href > 0:
            self.add_issue("info", "links", "empty_href_links",
                f"{empty_href} —Å—Å—ã–ª–æ–∫ —Å –ø—É—Å—Ç—ã–º href –∏–ª–∏ #",
                f"{empty_href} —à—Ç.", "0 —à—Ç.")

        return list(dict.fromkeys(internal_links)), list(dict.fromkeys(external_links))


# === SITE ANALYZER (Cross-page) ===

class SiteAnalyzer:
    """Cross-page analysis after all pages are crawled."""

    def __init__(self, result: SiteAuditResult,
                 internal_link_targets: Dict[str, Set[str]],
                 sitemap_urls: Set[str],
                 outgoing_links: Dict[str, Dict[str, Set[str]]],
                 unresolved_internal_status: Dict[str, int],
                 external_status: Dict[str, int],
                 raw_url_variants: Dict[str, Set[str]],
                 canonical_status_cache: Dict[str, int]):
        self.result = result
        self.link_targets = internal_link_targets
        self.sitemap_urls = sitemap_urls
        self.outgoing_links = outgoing_links
        self.unresolved_internal_status = unresolved_internal_status
        self.external_status = external_status
        self.raw_url_variants = raw_url_variants
        self.canonical_status_cache = canonical_status_cache

    def analyze(self):
        self._find_duplicate_titles()
        self._find_duplicate_descriptions()
        self._find_duplicate_content()
        self._find_orphan_pages()
        self._find_deep_pages()
        self._find_zero_inlinks()
        self._find_noindex_linked()
        self._build_redirect_map()
        self._build_broken_link_map()
        self._find_url_pattern_duplicates()
        self._find_thin_content_clusters()
        self._mark_robots_linked_blocked()
        self._check_no_structured_data()
        self._compare_sitemap()
        self._compute_interlinking_stats()
        self._compute_internal_pagerank()
        self._detect_canonical_chains()
        self._detect_canonical_sitemap_conflicts()
        self._validate_schema_markup()
        self._compute_status_distribution()
        self._compute_issue_stats()
        self._detect_frameworks()

    def _find_duplicate_titles(self):
        title_groups: Dict[str, List[str]] = defaultdict(list)
        for url, page in self.result.pages.items():
            if page.title:
                title_groups[page.title].append(url)

        self.result.duplicate_titles = [
            {"title": title, "urls": urls}
            for title, urls in title_groups.items()
            if len(urls) > 1
        ]
        for group in self.result.duplicate_titles:
            for url in group["urls"]:
                self.result.pages[url].issues.append(PageIssue(
                    "warning",
                    "content",
                    "duplicate_title",
                    "–î—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è Title –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö",
                    group["title"][:90],
                    "–£–Ω–∏–∫–∞–ª—å–Ω—ã–π Title",
                ))

    def _find_duplicate_descriptions(self):
        desc_groups: Dict[str, List[str]] = defaultdict(list)
        for url, page in self.result.pages.items():
            if page.description:
                desc_groups[page.description].append(url)

        self.result.duplicate_descriptions = [
            {"description": desc, "urls": urls}
            for desc, urls in desc_groups.items()
            if len(urls) > 1
        ]
        for group in self.result.duplicate_descriptions:
            for url in group["urls"]:
                self.result.pages[url].issues.append(PageIssue(
                    "warning",
                    "content",
                    "duplicate_description",
                    "–î—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è Meta Description",
                    group["description"][:90],
                    "–£–Ω–∏–∫–∞–ª—å–Ω—ã–π Description",
                ))

    def _find_duplicate_content(self):
        hash_groups: Dict[str, List[str]] = defaultdict(list)
        for url, page in self.result.pages.items():
            if page.content_hash:
                hash_groups[page.content_hash].append(url)

        duplicate_groups = [
            {"urls": urls}
            for h, urls in hash_groups.items()
            if len(urls) > 1
        ]

        # Approximate duplicates (>90%) for non-identical hashes.
        # Skip URLs already covered by exact-hash groups (M6 fix).
        already_in_exact = set()
        for group in duplicate_groups:
            already_in_exact.update(group["urls"])

        text_items = [
            (u, p.content_text)
            for u, p in self.result.pages.items()
            if p.content_text and u not in already_in_exact
        ]
        text_items.sort(key=lambda item: len(item[1]), reverse=True)
        similar_map: Dict[str, Set[str]] = defaultdict(set)
        comparisons = 0

        # Pre-compute signatures once (not inside inner loop)
        signatures: Dict[str, Set[str]] = {}
        for u, t in text_items:
            if len(t) >= 300:
                signatures[u] = set(re.findall(r"[a-z–∞-—è—ë0-9]{4,}", t[:1200].lower()))

        for i in range(len(text_items)):
            u1, t1 = text_items[i]
            if len(t1) < 300:
                continue
            sig1 = signatures.get(u1, set())
            for j in range(i + 1, len(text_items)):
                if comparisons >= MAX_SIMILARITY_PAIRS:
                    break
                u2, t2 = text_items[j]
                if len(t2) < 300:
                    continue
                if abs(len(t1) - len(t2)) > max(len(t1), len(t2)) * 0.25:
                    continue
                sig2 = signatures.get(u2, set())
                # Fast pre-filter by shingle overlap (Jaccard)
                if sig1 and sig2 and (len(sig1 & sig2) / max(1, len(sig1 | sig2))) < 0.18:
                    continue
                comparisons += 1
                # Use quick_ratio() first as O(1) pre-filter before full O(N¬≤) ratio()
                sm = SequenceMatcher(None, t1[:2000], t2[:2000])
                if sm.quick_ratio() < 0.85:
                    continue
                if sm.ratio() >= 0.9:
                    similar_map[u1].add(u2)
                    similar_map[u2].add(u1)
            if comparisons >= MAX_SIMILARITY_PAIRS:
                break

        seen = set()
        for url, neighbors in similar_map.items():
            if url in seen:
                continue
            cluster = {url}
            queue_sm = [url]
            while queue_sm:
                node = queue_sm.pop()
                if node in seen:
                    continue
                seen.add(node)
                cluster.add(node)
                for nxt in similar_map.get(node, set()):
                    if nxt not in seen:
                        queue_sm.append(nxt)
            if len(cluster) > 1:
                duplicate_groups.append({"urls": sorted(cluster)})

        self.result.duplicate_content = duplicate_groups
        for group in self.result.duplicate_content:
            for url in group["urls"]:
                self.result.pages[url].issues.append(PageIssue(
                    "warning",
                    "content",
                    "duplicate_content",
                    "–í—ã—Å–æ–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (>90%) —Å –¥—Ä—É–≥–∏–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏",
                ))

    def _find_orphan_pages(self):
        link_targets_norm = {self.normalize_url(u) for u in self.link_targets.keys()}
        base_norm = self.normalize_url(self.result.base_url)
        for sitemap_url in self.sitemap_urls:
            normed = self.normalize_url(sitemap_url)
            if normed == base_norm:
                continue
            if normed not in link_targets_norm:
                self.result.orphan_pages.append(sitemap_url)

    def _find_deep_pages(self):
        for url, page in self.result.pages.items():
            if page.crawl_depth > 3:
                self.result.deep_pages.append({
                    "url": url,
                    "depth": page.crawl_depth,
                })

    def _find_zero_inlinks(self):
        for url in self.result.pages.keys():
            sources = self.link_targets.get(url, set())
            if len(sources) == 0 and self.normalize_url(self.result.base_url) != url:
                self.result.zero_inlink_pages.append(url)
                self.result.pages[url].issues.append(PageIssue(
                    "warning",
                    "links",
                    "zero_inlinks",
                    "–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–µ –≤–µ–¥—É—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏",
                ))

    def _find_noindex_linked(self):
        for url, page in self.result.pages.items():
            if not page.is_indexable and len(self.link_targets.get(url, set())) > 0:
                self.result.noindex_linked_pages.append(url)

    def _build_redirect_map(self):
        for url, page in self.result.pages.items():
            if page.redirect_chain and len(page.redirect_chain) > 1:
                self.result.redirect_map.append({
                    "chain": page.redirect_chain,
                    "hops": len(page.redirect_chain) - 1,
                    "type": page.redirect_type,
                })

    def _build_broken_link_map(self):
        broken_internal_counter: Dict[str, int] = defaultdict(int)
        broken_external_counter: Dict[str, int] = defaultdict(int)

        for source_url, links in self.outgoing_links.items():
            for target in links["internal"]:
                if target in self.result.pages:
                    code = self.result.pages[target].status_code
                else:
                    code = self.unresolved_internal_status.get(target, 0)
                if code >= 400:
                    broken_internal_counter[source_url] += 1
                    self.result.broken_link_map.append({
                        "source": source_url,
                        "brokenUrl": target,
                        "statusCode": code,
                    })

            for ext in links["external"]:
                code = self.external_status.get(ext, 200)
                if self.external_status and code >= 400:
                    broken_external_counter[source_url] += 1

        for source, count in broken_internal_counter.items():
            page = self.result.pages.get(source)
            if not page:
                continue
            page.broken_internal_links = count
            page.issues.append(PageIssue(
                "critical",
                "links",
                "broken_internal_links",
                f"–ë–∏—Ç—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏: {count} —à—Ç.",
                str(count),
                "0",
            ))

        for source, count in broken_external_counter.items():
            page = self.result.pages.get(source)
            if not page or count <= 0:
                continue
            page.broken_external_links = count
            page.issues.append(PageIssue(
                "warning",
                "links",
                "broken_external_links",
                f"–ë–∏—Ç—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏: {count} —à—Ç.",
                str(count),
                "0",
            ))

    def _find_url_pattern_duplicates(self):
        for norm, variants in self.raw_url_variants.items():
            cleaned = sorted({v.rstrip("/") for v in variants})
            if len(cleaned) > 1:
                self.result.url_pattern_duplicates.append({
                    "normalized": norm,
                    "variants": cleaned[:10],
                })

    def _find_thin_content_clusters(self):
        clusters: Dict[str, List[str]] = defaultdict(list)
        for url, page in self.result.pages.items():
            if page.word_count < THIN_CONTENT_THRESHOLD:
                path = urlparse(url).path.strip("/")
                root = path.split("/")[0] if path else "/"
                clusters[root].append(url)
        for root, urls in clusters.items():
            if len(urls) >= 2:
                self.result.thin_content_clusters.append({"directory": root, "urls": urls})

    def _mark_robots_linked_blocked(self):
        # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ site-wide —Å–∏–≥–Ω–∞–ª –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.
        return

    def _check_no_structured_data(self):
        has_any_schema = any(p.has_schema or p.has_microdata for p in self.result.pages.values())
        self.result.no_structured_data = not has_any_schema

    def _compare_sitemap(self):
        crawled_normed = set()
        for url in self.result.pages:
            parsed = urlparse(url)
            normed = urlunparse((parsed.scheme, parsed.netloc, parsed.path.rstrip("/") or "/", "", "", ""))
            crawled_normed.add(normed)

        sitemap_normed = set()
        for url in self.sitemap_urls:
            parsed = urlparse(url)
            normed = urlunparse((parsed.scheme, parsed.netloc, parsed.path.rstrip("/") or "/", "", "", ""))
            sitemap_normed.add(normed)

        only_sitemap = list(sitemap_normed - crawled_normed)
        only_crawled = list(crawled_normed - sitemap_normed)

        self.result.sitemap_vs_crawled = {
            "onlyInSitemap": only_sitemap[:50],
            "onlyInCrawl": only_crawled[:50],
            "sitemapTotal": len(self.sitemap_urls),
            "crawledTotal": len(self.result.pages),
            "overlap": len(sitemap_normed & crawled_normed),
        }

    # === TOP1: Interlinking stats (inlink count, dead-ends, click depth) ===
    def _compute_interlinking_stats(self):
        base_norm = self.normalize_url(self.result.base_url)
        inlink_counts: Dict[str, int] = {}

        for url in self.result.pages:
            sources = self.link_targets.get(url, set())
            count = len(sources)
            inlink_counts[url] = count
            self.result.pages[url].inlink_count = count

        # Dead-end pages (no outgoing internal links)
        for url, page in self.result.pages.items():
            outgoing = self.outgoing_links.get(url, {})
            internal_out = outgoing.get("internal", set())
            if len(internal_out) == 0 and url != base_norm:
                page.is_dead_end = True
                self.result.dead_end_pages.append(url)
                page.issues.append(PageIssue(
                    "warning", "links", "dead_end_page",
                    "–¢—É–ø–∏–∫–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: –Ω–µ—Ç –∏—Å—Ö–æ–¥—è—â–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫",
                    "0 —Å—Å—ã–ª–æ–∫", ">0 —Å—Å—ã–ª–æ–∫"))

        # Stats
        counts = [c for u, c in inlink_counts.items() if u != base_norm]
        if counts:
            self.result.avg_inlinks = sum(counts) / len(counts)
            self.result.max_inlinks = max(counts)
            sorted_c = sorted(counts)
            mid = len(sorted_c) // 2
            self.result.median_inlinks = (sorted_c[mid] + sorted_c[~mid]) / 2

        # Pages with critically low inlinks (<2, excl. homepage)
        for url, count in inlink_counts.items():
            if url == base_norm:
                continue
            if count < 2:
                self.result.pages_low_inlinks.append(url)
                if count == 0:
                    pass  # already handled by _find_zero_inlinks
                else:
                    self.result.pages[url].issues.append(PageIssue(
                        "info", "links", "low_inlinks",
                        f"–ú–∞–ª–æ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫: {count} (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è ‚â•3)",
                        str(count), "‚â•3"))

    # === TOP2: Internal PageRank ===
    def _compute_internal_pagerank(self):
        """Simplified PageRank to show link equity distribution."""
        pages = list(self.result.pages.keys())
        if len(pages) < 2:
            return

        n = len(pages)
        idx = {url: i for i, url in enumerate(pages)}
        damping = 0.85
        scores = [1.0 / n] * n

        # Build outlink map: page -> list of internal pages it links to
        out_targets: List[List[int]] = [[] for _ in range(n)]
        for url in pages:
            outgoing = self.outgoing_links.get(url, {})
            for target in outgoing.get("internal", set()):
                if target in idx:
                    out_targets[idx[url]].append(idx[target])

        # 20 iterations is sufficient for convergence on small sites
        for _ in range(20):
            new_scores = [(1 - damping) / n] * n
            for i in range(n):
                if not out_targets[i]:
                    # Distribute dead-end "leaked" rank equally
                    share = damping * scores[i] / n
                    for j in range(n):
                        new_scores[j] += share
                else:
                    share = damping * scores[i] / len(out_targets[i])
                    for j in out_targets[i]:
                        new_scores[j] += share
            scores = new_scores

        # Normalize to 0-100 scale
        max_score = max(scores) if scores else 1
        for i, url in enumerate(pages):
            self.result.pages[url].internal_pagerank = round(
                (scores[i] / max(max_score, 1e-10)) * 100, 1)

    # === TOP5: Canonical chain detection ===
    def _detect_canonical_chains(self):
        """Detect A->B->C canonical chains (A's canonical points to B, B's canonical points to C)."""
        canonical_map: Dict[str, str] = {}
        for url, page in self.result.pages.items():
            if page.canonical and page.canonical_status == "other":
                canonical_map[url] = self.normalize_url(page.canonical)

        for url, target in canonical_map.items():
            if target in canonical_map and canonical_map[target] != target:
                chain = [url, target, canonical_map[target]]
                # Follow further
                seen = set(chain)
                current = canonical_map[target]
                while current in canonical_map and canonical_map[current] not in seen:
                    current = canonical_map[current]
                    chain.append(current)
                    seen.add(current)

                self.result.canonical_chains.append({
                    "chain": chain,
                    "length": len(chain),
                })
                self.result.pages[url].issues.append(PageIssue(
                    "warning", "technical", "canonical_chain",
                    f"–¶–µ–ø–æ—á–∫–∞ –∫–∞–Ω–æ–Ω–∏–∫–∞–ª–æ–≤: {len(chain)} —Ö–æ–ø–æ–≤",
                    " ‚Üí ".join(c.split("/")[-1] or "/" for c in chain[:4]),
                    "–ü—Ä—è–º–æ–π canonical –Ω–∞ –∫–æ–Ω–µ—á–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É"))

    # === TOP5: Canonical vs sitemap conflicts ===
    def _detect_canonical_sitemap_conflicts(self):
        """Detect pages in sitemap whose canonical points to a different URL."""
        sitemap_normed = set()
        for u in self.sitemap_urls:
            sitemap_normed.add(self.normalize_url(u))

        for url, page in self.result.pages.items():
            if url not in sitemap_normed:
                continue
            if not page.canonical or page.canonical_status != "other":
                continue
            canonical_norm = self.normalize_url(page.canonical)
            if canonical_norm != url:
                self.result.canonical_sitemap_conflicts.append({
                    "url": url,
                    "canonical": page.canonical,
                    "canonicalNorm": canonical_norm,
                })
                page.issues.append(PageIssue(
                    "warning", "technical", "canonical_sitemap_conflict",
                    "–°—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ sitemap, –Ω–æ canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–π URL",
                    page.canonical[:80], "canonical = URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã"))

    # === TOP4: Schema/JSON-LD validation ===
    def _validate_schema_markup(self):
        """Validate required fields in JSON-LD/Schema markup for each type."""
        REQUIRED_FIELDS = {
            "Article": ["headline", "author", "datePublished"],
            "NewsArticle": ["headline", "author", "datePublished"],
            "BlogPosting": ["headline", "author", "datePublished"],
            "Product": ["name", "offers"],
            "LocalBusiness": ["name", "address"],
            "Organization": ["name", "url"],
            "FAQPage": ["mainEntity"],
            "BreadcrumbList": ["itemListElement"],
            "WebSite": ["name", "url"],
            "Event": ["name", "startDate", "location"],
        }

        for url, page in self.result.pages.items():
            if not page.has_schema:
                continue
            # Parse JSON-LD from raw HTML
            html_text = page.raw_html
            if not html_text:
                continue
            try:
                soup_local = BeautifulSoup(html_text[:50000], "lxml")
            except Exception:
                continue

            for script in soup_local.find_all("script", type="application/ld+json"):
                try:
                    import json
                    data = json.loads(script.string or "")
                    items = data if isinstance(data, list) else [data]
                    for item in items:
                        schema_type = item.get("@type", "")
                        if isinstance(schema_type, list):
                            schema_type = schema_type[0] if schema_type else ""
                        required = REQUIRED_FIELDS.get(schema_type, [])
                        missing = [f for f in required if f not in item or not item[f]]
                        if missing:
                            self.result.schema_validation_issues.append({
                                "url": url,
                                "type": schema_type,
                                "missing": missing,
                            })
                            page.issues.append(PageIssue(
                                "warning", "structured_data", "schema_incomplete",
                                f"Schema {schema_type}: –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è",
                                ", ".join(missing),
                                "–í—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω—ã"))
                except (json.JSONDecodeError, TypeError, AttributeError):
                    self.result.schema_validation_issues.append({
                        "url": url,
                        "type": "JSON-LD",
                        "missing": ["valid_json"],
                    })
                    page.issues.append(PageIssue(
                        "warning", "structured_data", "schema_invalid_json",
                        "–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON-LD —Ä–∞–∑–º–µ—Ç–∫–∏ Schema.org",
                        "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞", "–í–∞–ª–∏–¥–Ω—ã–π JSON"))

    def _compute_status_distribution(self):
        codes: Dict[str, int] = defaultdict(int)
        for page in self.result.pages.values():
            codes[str(page.status_code)] = codes.get(str(page.status_code), 0) + 1
        self.result.status_codes = dict(codes)

    def _compute_issue_stats(self):
        by_severity: Dict[str, int] = defaultdict(int)
        by_category: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))

        for page in self.result.pages.values():
            for issue in page.issues:
                by_severity[issue.severity] += 1
                by_category[issue.category][issue.severity] += 1

        self.result.issues_by_severity = dict(by_severity)
        self.result.issues_by_category = {k: dict(v) for k, v in by_category.items()}

    def _detect_frameworks(self):
        frameworks = set()
        for page in self.result.pages.values():
            if page.detected_framework:
                frameworks.add(page.detected_framework)
        self.result.detected_frameworks = list(frameworks)

    def normalize_url(self, url: str) -> str:
        try:
            parsed = urlparse(url)
            path = parsed.path.rstrip("/") or "/"
            return urlunparse((parsed.scheme.lower(), parsed.netloc.lower(), path, "", parsed.query, ""))
        except Exception:
            return url


# === REPORT GENERATOR (Scoring + Recommendations) ===

class ReportGenerator:
    """Calculates scores and generates recommendations."""

    def __init__(self, result: SiteAuditResult):
        self.result = result

    def calculate_scores(self):
        if not self.result.pages:
            return

        # Category config: issue_categories, weight in final score
        # Weights reflect actual SEO impact:
        #   technical/links are most important for crawlability/indexing
        #   content directly affects rankings
        #   images/structured_data are secondary signals
        categories = {
            "technical": {"keys": ["technical", "security"], "weight": 0.30},
            "content":   {"keys": ["content"],              "weight": 0.25},
            "links":     {"keys": ["links"],                "weight": 0.25},
            "images":    {"keys": ["images"],               "weight": 0.10},
            "structured_data": {"keys": ["structured_data"], "weight": 0.10},
        }

        pages_count = max(1, self.result.total_scanned)
        category_penalties: Dict[str, float] = {}
        category_raw_penalties: Dict[str, int] = {}
        for cat_name, cat_cfg in categories.items():
            cat_penalty = 0
            for page in self.result.pages.values():
                for issue in page.issues:
                    if issue.category in cat_cfg["keys"]:
                        cat_penalty += PENALTY_WEIGHTS.get(issue.severity, 0)
            category_raw_penalties[cat_name] = cat_penalty
            # Normalize to site size, cap at MAX_CATEGORY_PENALTY
            normalized = min(MAX_CATEGORY_PENALTY, (cat_penalty / pages_count) * 6)
            category_penalties[cat_name] = normalized
            cat_score = max(0, int(100 - (normalized / MAX_CATEGORY_PENALTY) * 100))
            self.result.category_scores[cat_name] = cat_score

        # Weighted average instead of simple average
        if self.result.category_scores:
            weighted_sum = 0.0
            total_weight = 0.0
            for cat_name, cat_cfg in categories.items():
                score = self.result.category_scores.get(cat_name, 100)
                weighted_sum += score * cat_cfg["weight"]
                total_weight += cat_cfg["weight"]
            self.result.health_score = int(max(0, min(100, round(weighted_sum / max(total_weight, 0.01)))))
        else:
            self.result.health_score = 0

        # Score explanation for transparency
        self.result.score_explanation = {
            "formula": "weighted_average(category_scores * weights)",
            "weights": {k: v["weight"] for k, v in categories.items()},
            "category_scores": dict(self.result.category_scores),
            "category_raw_penalties": category_raw_penalties,
            "pages_count": pages_count,
            "health_score": self.result.health_score,
        }

    def generate_recommendations(self):
        """Group issues into actionable recommendations."""
        recs: List[Dict] = []
        issue_code_urls: Dict[str, List[str]] = defaultdict(list)

        for url, page in self.result.pages.items():
            for issue in page.issues:
                issue_code_urls[issue.code].append(url)

        # Map issue codes to recommendation templates
        code_to_rec = {
            "client_error": "broken_links",
            "server_error": "broken_links",
            "broken_internal_links": "broken_links",
            "missing_h1": "missing_h1",
            "empty_h1": "missing_h1",
            "multiple_h1": "multiple_h1",
            "missing_title": "missing_title",
            "empty_title": "empty_title",
            "missing_description": "missing_description",
            "empty_description": "missing_description",
            "missing_alt": "missing_alt",
            "missing_canonical": "missing_canonical",
            "slow_ttfb": "slow_pages",
            "no_schema": "no_schema",
            "redirect_chain": "redirect_chains",
            "missing_viewport": "missing_viewport",
            "thin_content": "thin_content",
            "very_thin_content": "thin_content",
            "missing_og_title": "missing_og",
            "missing_og_desc": "missing_og",
            "missing_og_image": "missing_og",
            "missing_charset": "missing_charset",
            "mixed_content": "mixed_content",
            "heading_hierarchy": "heading_hierarchy",
            "canonical_target_error": "missing_canonical",
            "canonical_not_self": "missing_canonical",
            "broken_external_links": "broken_links",
        }

        seen_recs = set()
        for code, urls in issue_code_urls.items():
            rec_key = code_to_rec.get(code)
            if not rec_key or rec_key in seen_recs:
                continue
            seen_recs.add(rec_key)

            template = RECOMMENDATIONS_RU.get(rec_key)
            if not template:
                continue

            # Determine severity from the issues
            severity = "info"
            for page in self.result.pages.values():
                for issue in page.issues:
                    if issue.code == code:
                        if SEVERITY_ORDER.get(issue.severity, 2) < SEVERITY_ORDER.get(severity, 2):
                            severity = issue.severity

            unique_urls = list(set(urls))[:20]
            count = len(set(urls))

            recs.append({
                "key": rec_key,
                "title": template["title"].format(count=count),
                "impact": template["impact"],
                "fix": template["fix"],
                "effort": template["effort"],
                "severity": severity,
                "urls": unique_urls,
                "count": count,
            })

        # Add site-level checks
        if not self.result.http_to_https:
            recs.append({
                "key": "no_https_redirect",
                "title": RECOMMENDATIONS_RU["no_https_redirect"]["title"],
                "impact": RECOMMENDATIONS_RU["no_https_redirect"]["impact"],
                "fix": RECOMMENDATIONS_RU["no_https_redirect"]["fix"],
                "effort": "quick",
                "severity": "critical",
                "urls": [self.result.base_url],
                "count": 1,
            })

        if self.result.orphan_pages:
            count = len(self.result.orphan_pages)
            recs.append({
                "key": "orphan_pages",
                "title": RECOMMENDATIONS_RU["orphan_pages"]["title"].format(count=count),
                "impact": RECOMMENDATIONS_RU["orphan_pages"]["impact"],
                "fix": RECOMMENDATIONS_RU["orphan_pages"]["fix"],
                "effort": "medium",
                "severity": "warning",
                "urls": self.result.orphan_pages[:20],
                "count": count,
            })

        if self.result.duplicate_titles:
            count = len(self.result.duplicate_titles)
            recs.append({
                "key": "duplicate_titles",
                "title": RECOMMENDATIONS_RU["duplicate_titles"]["title"].format(count=count),
                "impact": RECOMMENDATIONS_RU["duplicate_titles"]["impact"],
                "fix": RECOMMENDATIONS_RU["duplicate_titles"]["fix"],
                "effort": "medium",
                "severity": "warning",
                "urls": [u for g in self.result.duplicate_titles for u in g["urls"]][:20],
                "count": count,
            })

        if self.result.duplicate_descriptions:
            count = len(self.result.duplicate_descriptions)
            recs.append({
                "key": "duplicate_descriptions",
                "title": RECOMMENDATIONS_RU["duplicate_descriptions"]["title"].format(count=count),
                "impact": RECOMMENDATIONS_RU["duplicate_descriptions"]["impact"],
                "fix": RECOMMENDATIONS_RU["duplicate_descriptions"]["fix"],
                "effort": "medium",
                "severity": "warning",
                "urls": [u for g in self.result.duplicate_descriptions for u in g["urls"]][:20],
                "count": count,
            })

        if self.result.no_structured_data:
            recs.append({
                "key": "no_schema",
                "title": RECOMMENDATIONS_RU["no_schema"]["title"],
                "impact": RECOMMENDATIONS_RU["no_schema"]["impact"],
                "fix": RECOMMENDATIONS_RU["no_schema"]["fix"],
                "effort": "complex",
                "severity": "warning",
                "urls": [self.result.base_url],
                "count": 1,
            })

        if self.result.noindex_linked_pages:
            count = len(self.result.noindex_linked_pages)
            template = RECOMMENDATIONS_RU["noindex_linked"]
            recs.append({
                "key": "noindex_linked",
                "title": template["title"].format(count=count),
                "impact": template["impact"],
                "fix": template["fix"],
                "effort": template["effort"],
                "severity": "warning",
                "urls": self.result.noindex_linked_pages[:20],
                "count": count,
            })

        if self.result.robots_linked_blocked:
            recs.append({
                "key": "robots_blocked_linked",
                "title": f"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ URL, –∑–∞–∫—Ä—ã—Ç—ã–µ robots.txt ({len(self.result.robots_linked_blocked)} —à—Ç.)",
                "impact": "–°—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–∫—Ä—ã—Ç—ã–µ URL —Ç—Ä–∞—Ç—è—Ç –∫—Ä–∞—É–ª–∏–Ω–≥–æ–≤—ã–π –±—é–¥–∂–µ—Ç –∏ –º–µ—à–∞—é—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤–∞–∂–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.",
                "fix": "–û—Ç–∫—Ä–æ–π—Ç–µ –≤–∞–∂–Ω—ã–µ URL –≤ robots.txt –∏–ª–∏ —É–±–µ—Ä–∏—Ç–µ –Ω–∞ –Ω–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏.",
                "effort": "medium",
                "severity": "warning",
                "urls": self.result.robots_linked_blocked[:20],
                "count": len(self.result.robots_linked_blocked),
            })

        if not self.result.robots_has_clean_param and any(
            p.url for p in self.result.pages.values()
            if "?" in p.url and any(k in p.url for k in ["utm_", "sort=", "filter="])
        ):
            recs.append({
                "key": "yandex_clean_param",
                "title": RECOMMENDATIONS_RU["yandex_clean_param"]["title"],
                "impact": RECOMMENDATIONS_RU["yandex_clean_param"]["impact"],
                "fix": RECOMMENDATIONS_RU["yandex_clean_param"]["fix"],
                "effort": "quick",
                "severity": "info",
                "urls": [],
                "count": 0,
            })

        # Dead-end pages
        if self.result.dead_end_pages:
            count = len(self.result.dead_end_pages)
            template = RECOMMENDATIONS_RU["dead_end_pages"]
            recs.append({
                "key": "dead_end_pages",
                "title": template["title"].format(count=count),
                "impact": template["impact"],
                "fix": template["fix"],
                "effort": template["effort"],
                "severity": "warning",
                "urls": self.result.dead_end_pages[:20],
                "count": count,
            })

        # Low inlinks
        if self.result.pages_low_inlinks:
            count = len(self.result.pages_low_inlinks)
            template = RECOMMENDATIONS_RU["low_inlinks"]
            recs.append({
                "key": "low_inlinks",
                "title": template["title"].format(count=count),
                "impact": template["impact"],
                "fix": template["fix"],
                "effort": template["effort"],
                "severity": "warning",
                "urls": self.result.pages_low_inlinks[:20],
                "count": count,
            })

        # Canonical chains
        if self.result.canonical_chains:
            count = len(self.result.canonical_chains)
            template = RECOMMENDATIONS_RU["canonical_chain"]
            recs.append({
                "key": "canonical_chain",
                "title": template["title"].format(count=count),
                "impact": template["impact"],
                "fix": template["fix"],
                "effort": template["effort"],
                "severity": "warning",
                "urls": [c["chain"][0] for c in self.result.canonical_chains[:20]],
                "count": count,
            })

        # Canonical-sitemap conflicts
        if self.result.canonical_sitemap_conflicts:
            count = len(self.result.canonical_sitemap_conflicts)
            template = RECOMMENDATIONS_RU["canonical_sitemap_conflict"]
            recs.append({
                "key": "canonical_sitemap_conflict",
                "title": template["title"].format(count=count),
                "impact": template["impact"],
                "fix": template["fix"],
                "effort": template["effort"],
                "severity": "warning",
                "urls": [c["url"] for c in self.result.canonical_sitemap_conflicts[:20]],
                "count": count,
            })

        # Schema validation
        if self.result.schema_validation_issues:
            count = len(set(i["url"] for i in self.result.schema_validation_issues))
            template = RECOMMENDATIONS_RU["schema_incomplete"]
            recs.append({
                "key": "schema_incomplete",
                "title": template["title"].format(count=count),
                "impact": template["impact"],
                "fix": template["fix"],
                "effort": template["effort"],
                "severity": "warning",
                "urls": list(set(i["url"] for i in self.result.schema_validation_issues))[:20],
                "count": count,
            })

        # Sort by severity
        recs.sort(key=lambda x: SEVERITY_ORDER.get(x["severity"], 2))
        self.result.recommendations = recs

    def to_excel(self) -> bytes:
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            # Sheet 1: Summary
            summary = pd.DataFrame([{
                "–ú–µ—Ç—Ä–∏–∫–∞": "–ó–¥–æ—Ä–æ–≤—å–µ —Å–∞–π—Ç–∞",
                "–ó–Ω–∞—á–µ–Ω–∏–µ": f"{self.result.health_score}/100",
            }, {
                "–ú–µ—Ç—Ä–∏–∫–∞": "–ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü",
                "–ó–Ω–∞—á–µ–Ω–∏–µ": str(self.result.total_scanned),
            }, {
                "–ú–µ—Ç—Ä–∏–∫–∞": "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è",
                "–ó–Ω–∞—á–µ–Ω–∏–µ": f"{self.result.duration}—Å",
            }, {
                "–ú–µ—Ç—Ä–∏–∫–∞": "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫",
                "–ó–Ω–∞—á–µ–Ω–∏–µ": str(self.result.issues_by_severity.get("critical", 0)),
            }, {
                "–ú–µ—Ç—Ä–∏–∫–∞": "–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π",
                "–ó–Ω–∞—á–µ–Ω–∏–µ": str(self.result.issues_by_severity.get("warning", 0)),
            }, {
                "–ú–µ—Ç—Ä–∏–∫–∞": "–î–∞—Ç–∞ –∞—É–¥–∏—Ç–∞",
                "–ó–Ω–∞—á–µ–Ω–∏–µ": datetime.now().strftime("%Y-%m-%d %H:%M"),
            }])
            summary.to_excel(writer, sheet_name="–°–≤–æ–¥–∫–∞", index=False)

            # Sheet 2: All pages
            pages_data = []
            for url, page in self.result.pages.items():
                pages_data.append({
                    "URL": url,
                    "–ö–æ–¥": page.status_code,
                    "Title": page.title[:80],
                    "–î–ª. Title": page.title_length,
                    "–û–ø–∏—Å–∞–Ω–∏–µ": page.description[:80],
                    "–î–ª. Description": page.description_length,
                    "H1 (–∫–æ–ª-–≤–æ)": page.h1_count,
                    "–°–ª–æ–≤": page.word_count,
                    "–ö–∞—Ä—Ç–∏–Ω–∫–∏ –±–µ–∑ ALT": page.images_missing_alt,
                    "TTFB (—Å–µ–∫)": round(page.ttfb, 2),
                    "Canonical": page.canonical_status,
                    "–í—Ö. —Å—Å—ã–ª–æ–∫": page.inlink_count,
                    "PageRank": page.internal_pagerank,
                    "–¢—É–ø–∏–∫": "–î–∞" if page.is_dead_end else "",
                    "–û—à–∏–±–æ–∫": len(page.issues),
                })
            pd.DataFrame(pages_data).to_excel(writer, sheet_name="–í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", index=False)

            # Sheet 3: All issues
            issues_data = []
            for url, page in self.result.pages.items():
                for issue in page.issues:
                    issues_data.append({
                        "URL": url,
                        "–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å": issue.severity,
                        "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": issue.category,
                        "–ü—Ä–æ–±–ª–µ–º–∞": issue.message,
                        "–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ": issue.current_value,
                        "–ù–æ—Ä–º–∞": issue.expected_value,
                    })
            pd.DataFrame(issues_data).to_excel(writer, sheet_name="–í—Å–µ –æ—à–∏–±–∫–∏", index=False)

            # Sheet 4: Action plan
            plan_data = []
            for rec in self.result.recommendations:
                plan_data.append({
                    "–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç": rec["severity"],
                    "–ó–∞–¥–∞—á–∞": rec["title"],
                    "–í–ª–∏—è–Ω–∏–µ": rec["impact"],
                    "–ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å": rec["fix"],
                    "–°–ª–æ–∂–Ω–æ—Å—Ç—å": rec["effort"],
                    "–ö–æ–ª-–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü": rec["count"],
                    "–ü—Ä–∏–º–µ—Ä—ã URL": "; ".join(rec["urls"][:5]),
                })
            pd.DataFrame(plan_data).to_excel(writer, sheet_name="–ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π", index=False)

            # Sheet 5: Duplicates
            dupes_data = []
            for group in self.result.duplicate_titles:
                for url in group["urls"]:
                    dupes_data.append({
                        "–¢–∏–ø": "Title",
                        "–ó–Ω–∞—á–µ–Ω–∏–µ": group["title"][:80],
                        "URL": url,
                    })
            for group in self.result.duplicate_descriptions:
                for url in group["urls"]:
                    dupes_data.append({
                        "–¢–∏–ø": "Description",
                        "–ó–Ω–∞—á–µ–Ω–∏–µ": group["description"][:80],
                        "URL": url,
                    })
            for group in self.result.duplicate_content:
                for url in group["urls"]:
                    dupes_data.append({
                        "–¢–∏–ø": "–ö–æ–Ω—Ç–µ–Ω—Ç",
                        "–ó–Ω–∞—á–µ–Ω–∏–µ": "(>90% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)",
                        "URL": url,
                    })
            pd.DataFrame(dupes_data).to_excel(writer, sheet_name="–î—É–±–ª–∏", index=False)

        return output.getvalue()

    def to_csv(self) -> str:
        rows = []
        for url, page in self.result.pages.items():
            rows.append({
                "URL": url,
                "–ö–æ–¥": page.status_code,
                "Title": page.title,
                "–î–ª. Title": page.title_length,
                "–û–ø–∏—Å–∞–Ω–∏–µ": page.description,
                "–î–ª. Description": page.description_length,
                "H1": "; ".join(page.h1_list),
                "–°–ª–æ–≤": page.word_count,
                "TTFB": round(page.ttfb, 2),
                "–û—à–∏–±–æ–∫": len(page.issues),
            })
        return pd.DataFrame(rows).to_csv(index=False)


# === STREAMLIT UI ===

def inject_custom_css() -> None:
    """–í–Ω–µ–¥—Ä—è–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—É—é —Ç—ë–º–Ω—É—é —Ç–µ–º—É –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."""
    st.markdown(
        """
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap');
            html, body, [class*="css"] { font-family: 'Inter', sans-serif; }
            #MainMenu, footer, header { visibility: hidden; }

            [data-testid="stAppViewContainer"] {
                background: radial-gradient(1400px 600px at 50% -20%, #1F2A52 0%, #0F172A 48%);
            }
            [data-testid="stSidebar"] {
                background-color: #0F172A;
                border-right: 1px solid #334155;
            }
            .stTabs [data-baseweb="tab-list"] {
                gap: 0px;
                background-color: #1E293B;
                border-radius: 12px;
                padding: 4px;
            }
            .stTabs [data-baseweb="tab"] {
                border-radius: 8px;
                color: #94A3B8;
                font-weight: 500;
                padding: 8px 16px;
            }
            .stTabs [aria-selected="true"] {
                background-color: #6366F1 !important;
                color: #FFFFFF !important;
            }
            .stButton > button[kind="primary"] {
                background-color: #6366F1;
                border: none;
                border-radius: 8px;
                font-weight: 600;
                padding: 10px 20px;
                width: 100%;
            }
            .stButton > button[kind="primary"]:hover {
                background-color: #4338CA;
            }
            .metric-card {
                background: #1E293B;
                border: 1px solid #334155;
                border-radius: 12px;
                padding: 20px;
                text-align: center;
                transition: border-color 0.2s ease;
            }
            .metric-card:hover { border-color: #6366F1; }
            .recommendation-card {
                background: #1E293B;
                border: 1px solid #334155;
                border-left: 4px solid;
                border-radius: 0 12px 12px 0;
                padding: 16px 20px;
                margin-bottom: 12px;
            }
            .scan-log-entry {
                font-family: 'JetBrains Mono', monospace;
                font-size: 13px;
                padding: 4px 8px;
                border-bottom: 1px solid #1E293B;
            }
            .scan-log-entry.ok { color: #10B981; }
            .scan-log-entry.error { color: #EF4444; }
            .scan-log-entry.info { color: #94A3B8; }
            .scan-log-entry.active { color: #F59E0B; background: rgba(245,158,11,0.05); }
            @media (max-width: 768px) {
                .metric-card { padding: 12px; }
                [data-testid="stHorizontalBlock"] { flex-wrap: wrap; }
            }
        </style>
        """,
        unsafe_allow_html=True,
    )


def apply_chart_style(fig: go.Figure, title: str = "") -> None:
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –µ–¥–∏–Ω—ã–π —Å—Ç–∏–ª—å –∫ Plotly-–≥—Ä–∞—Ñ–∏–∫–∞–º."""
    fig.update_layout(
        **PLOTLY_LAYOUT,
        title=dict(
            text=title,
            font=dict(size=16, color="#F8FAFC"),
            x=0,
            xanchor="left",
        ),
        legend=dict(
            bgcolor="rgba(0,0,0,0)",
            borderwidth=0,
            font=dict(size=12, color="#94A3B8"),
            orientation="h",
            yanchor="bottom",
            y=-0.2,
            xanchor="center",
            x=0.5,
        ),
        hoverlabel=dict(
            bgcolor="#1E293B",
            bordercolor="#475569",
            font=dict(size=13, color="#F8FAFC"),
        ),
    )


def severity_badge(level: str) -> str:
    """HTML-–±–µ–π–¥–∂ —Å–µ—Ä—å—ë–∑–Ω–æ—Å—Ç–∏."""
    config = {
        "critical": ("üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è", COLORS["critical"], COLORS["critical_bg"]),
        "warning": ("üü° –í–Ω–∏–º–∞–Ω–∏–µ", COLORS["warning"], COLORS["warning_bg"]),
        "info": ("üîµ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", COLORS["info"], COLORS["info_bg"]),
    }
    label, text_color, bg_color = config.get(level, config["info"])
    return (
        f'<span style="background:{bg_color}; color:{text_color}; '
        f'padding:2px 8px; border-radius:6px; font-size:12px; font-weight:600;">{label}</span>'
    )


@st.cache_data(show_spinner=False)
def build_pages_dataframe(_audit_id: str, pages_payload: Dict[str, Dict[str, Any]]) -> pd.DataFrame:
    """–ö—ç—à–∏—Ä—É–µ–º–∞—è —Å–±–æ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü—ã –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü. _audit_id is the cheap cache key."""
    canonical_map = {
        "ok": "‚úÖ –û–ö",
        "missing": "‚ö†Ô∏è –ù–µ—Ç",
        "error": "üî¥ –û—à–∏–±–∫–∞",
        "other": "‚ö†Ô∏è –ù–∞ –¥—Ä—É–≥–æ–π URL",
    }
    rows = []
    for url, page in pages_payload.items():
        rows.append({
            "URL": url,
            "–ö–æ–¥": page.get("statusCode", 0),
            "Title": (page.get("title") or "")[:50],
            "–î–ª. Title": page.get("titleLength", 0),
            "–î–ª. –û–ø–∏—Å–∞–Ω–∏—è": page.get("descriptionLength", 0),
            "H1 (–∫–æ–ª-–≤–æ)": page.get("h1Count", 0),
            "–°–ª–æ–≤": page.get("wordCount", 0),
            "–ö–∞—Ä—Ç–∏–Ω–∫–∏ –±–µ–∑ ALT": page.get("imagesMissingAlt", 0),
            "TTFB (—Å–µ–∫)": round(page.get("ttfb", 0.0), 2),
            "Canonical": canonical_map.get(page.get("canonicalStatus", "missing"), "‚ö†Ô∏è –ù–µ—Ç"),
            "–û—à–∏–±–æ–∫": len(page.get("issues", [])),
            "–ë–∏—Ç—ã–µ –≤–Ω—É—Ç—Ä. —Å—Å—ã–ª–∫–∏": page.get("brokenInternalLinks", 0),
        })
    return pd.DataFrame(rows)


@st.cache_data(show_spinner=False)
def build_issues_dataframe(_audit_id: str, pages_payload: Dict[str, Dict[str, Any]]) -> pd.DataFrame:
    """–ö—ç—à–∏—Ä—É–µ–º–∞—è —Å–±–æ—Ä–∫–∞ –ø–ª–æ—Å–∫–æ–π —Ç–∞–±–ª–∏—Ü—ã –æ—à–∏–±–æ–∫. _audit_id is the cheap cache key."""
    rows = []
    for url, page in pages_payload.items():
        for issue in page.get("issues", []):
            rows.append({
                "URL": url,
                "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": issue.get("category", ""),
                "–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å": issue.get("severity", ""),
                "–ü—Ä–æ–±–ª–µ–º–∞": issue.get("message", ""),
                "–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ": issue.get("currentValue", ""),
                "–ù–æ—Ä–º–∞": issue.get("expectedValue", ""),
                "–ö–æ–¥": issue.get("code", ""),
            })
    if not rows:
        return pd.DataFrame(columns=["URL", "–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å", "–ü—Ä–æ–±–ª–µ–º–∞", "–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ", "–ù–æ—Ä–º–∞", "–ö–æ–¥"])
    return pd.DataFrame(rows)


class SEOAuditApp:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""

    def __init__(self):
        self.setup_page()
        inject_custom_css()
        self.init_session_state()

    def setup_page(self) -> None:
        st.set_page_config(
            page_title="SEO –ê—É–¥–∏—Ç-–ú–∞—à–∏–Ω–∞",
            page_icon="üîç",
            layout="wide",
            initial_sidebar_state="expanded",
        )

    def init_session_state(self) -> None:
        defaults = {
            "state": "IDLE",
            "results": None,
            "error_message": "",
            "scan_log": [],
            "scan_stats": {"progress": 0.0, "found": 0, "done": 0, "queue": 0, "errors": 0, "elapsed": 0.0},
            "session_id": str(uuid.uuid4()),
            "is_authenticated": False,
            "auth_error": "",
            "audit_id": "",
            "config": {
                "url": "",
                "max_pages": UI_DEFAULT_MAX_PAGES,
                "max_depth": 5,
                "crawl_delay": 0.3,
                "respect_robots": True,
                "check_external": False,
            },
        }
        for key, value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = value

    def render(self) -> None:
        self._sync_auth_state()
        self.render_sidebar()

        if not st.session_state.get("is_authenticated", False):
            self.render_login_screen()
            return

        state = st.session_state["state"]
        if state == "IDLE":
            self.render_welcome_screen()
        elif state == "SCANNING":
            self.render_scanning_screen()
        elif state == "RESULTS":
            self.render_results()
        elif state == "ERROR":
            self.render_error_screen()

    def _sync_auth_state(self) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ —Å–Ω–∏–º–∞–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø–æ TTL."""
        guard = get_auth_guard()
        sid = st.session_state.get("session_id", "")
        now = time.time()
        with guard.lock:
            if guard.active_session_id and (now - guard.last_seen) > AUTH_TTL_SECONDS:
                guard.active_session_id = ""
                guard.active_username = ""
                guard.last_seen = 0.0

            active_sid = guard.active_session_id

        if st.session_state.get("is_authenticated", False):
            if active_sid != sid:
                st.session_state["is_authenticated"] = False
            else:
                with guard.lock:
                    guard.last_seen = now

    def _try_login(self, username: str, password: str) -> None:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—É—â—É—é —Å–µ—Å—Å–∏—é. –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–∞—è."""
        username = (username or "").strip()
        password = password or ""
        if username != AUTH_USERNAME or password != AUTH_PASSWORD:
            st.session_state["auth_error"] = "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ª–æ–≥–∏–Ω –∏–ª–∏ –ø–∞—Ä–æ–ª—å."
            return

        guard = get_auth_guard()
        sid = st.session_state.get("session_id", "")
        now = time.time()
        with guard.lock:
            # TTL-safe: –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Å–µ—Å—Å–∏—è "—É–º–µ—Ä–ª–∞", –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Å–ª–æ—Ç.
            if guard.active_session_id and (now - guard.last_seen) > AUTH_TTL_SECONDS:
                guard.active_session_id = ""
                guard.active_username = ""
                guard.last_seen = 0.0

            if guard.active_session_id and guard.active_session_id != sid:
                st.session_state["auth_error"] = (
                    "‚è≥ –ü—Ä–æ—Ñ–∏–ª—å —Å–µ–π—á–∞—Å –∑–∞–Ω—è—Ç –¥—Ä—É–≥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                )
                return

            guard.active_session_id = sid
            guard.active_username = username
            guard.last_seen = now

        st.session_state["is_authenticated"] = True
        st.session_state["auth_error"] = ""
        st.rerun()

    def _logout(self) -> None:
        guard = get_auth_guard()
        sid = st.session_state.get("session_id", "")
        with guard.lock:
            if guard.active_session_id == sid:
                guard.active_session_id = ""
                guard.active_username = ""
                guard.last_seen = 0.0
        st.session_state["is_authenticated"] = False
        st.session_state["state"] = "IDLE"
        st.session_state["results"] = None
        st.session_state["error_message"] = ""
        st.session_state["auth_error"] = ""
        st.rerun()

    def render_sidebar(self) -> None:
        with st.sidebar:
            st.markdown("## üîç SEO –ê—É–¥–∏—Ç-–ú–∞—à–∏–Ω–∞")
            st.markdown("---")

            cfg = st.session_state["config"]
            cfg["max_pages"] = max(10, min(int(cfg.get("max_pages", UI_DEFAULT_MAX_PAGES)), UI_MAX_PAGES_LIMIT))
            cfg["max_depth"] = max(1, min(int(cfg.get("max_depth", 5)), 10))
            audit_guard = get_runtime_guard()
            auth_guard = get_auth_guard()

            if not st.session_state.get("is_authenticated", False):
                st.markdown("### üîê –í—Ö–æ–¥")
                username = st.text_input("–õ–æ–≥–∏–Ω", key="auth_username", help="–õ–æ–≥–∏–Ω –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ—Ä–≤–∏—Å—É.")
                password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="auth_password", help="–ü–∞—Ä–æ–ª—å –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ—Ä–≤–∏—Å—É.")
                if st.button("üîê –í–æ–π—Ç–∏", type="primary"):
                    self._try_login(username, password)
                err = st.session_state.get("auth_error", "")
                if err:
                    if "–∑–∞–Ω—è—Ç" in err:
                        st.warning(err)
                    else:
                        st.error(err)

                with auth_guard.lock:
                    if auth_guard.active_session_id and auth_guard.active_session_id != st.session_state["session_id"]:
                        idle_for = max(0, int(time.time() - auth_guard.last_seen))
                        st.caption(f"–ê–∫—Ç–∏–≤–Ω–∞—è —Å–µ—Å—Å–∏—è –¥—Ä—É–≥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ‚Ä¢ –ø–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {idle_for} —Å–µ–∫ –Ω–∞–∑–∞–¥.")
                st.markdown("---")
                st.caption("v1.0 ‚Ä¢ Python + Streamlit")
                return

            with auth_guard.lock:
                active_user = auth_guard.active_username or AUTH_USERNAME
            st.success(f"üë§ –í—Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω: {active_user}")
            if st.button("üö™ –í—ã–π—Ç–∏", use_container_width=True):
                self._logout()

            url_value = st.text_input(
                "–ê–¥—Ä–µ—Å —Å–∞–π—Ç–∞:",
                value=cfg["url"],
                placeholder="https://example.com",
                help="–í–≤–µ–¥–∏—Ç–µ –ø–æ–ª–Ω—ã–π URL —Å–∞–π—Ç–∞ –¥–ª—è –∞—É–¥–∏—Ç–∞.",
            )
            cfg["url"] = url_value.strip()

            if audit_guard.is_busy(st.session_state["session_id"]):
                st.info("‚è≥ –°–µ–π—á–∞—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∞—É–¥–∏—Ç –¥—Ä—É–≥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ó–∞–ø—É—Å–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

            if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞—É–¥–∏—Ç", type="primary"):
                self.start_scan()

            with st.expander("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏"):
                cfg["max_pages"] = st.slider(
                    "–ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü",
                    10,
                    UI_MAX_PAGES_LIMIT,
                    int(cfg["max_pages"]),
                    help="–°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å. –î–ª—è –º–∞–ª–æ–≥–æ —Å–∞–π—Ç–∞ –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 50.",
                )
                cfg["max_depth"] = st.slider(
                    "–ì–ª—É–±–∏–Ω–∞",
                    1,
                    10,
                    int(cfg["max_depth"]),
                    help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å—Å—ã–ª–∫–∞–º –æ—Ç –≥–ª–∞–≤–Ω–æ–π.",
                )
                cfg["crawl_delay"] = st.slider(
                    "–ó–∞–¥–µ—Ä–∂–∫–∞ (—Å–µ–∫)",
                    0.1,
                    2.0,
                    float(cfg["crawl_delay"]),
                    step=0.1,
                    help="–ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∏—Å–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫.",
                )
                cfg["respect_robots"] = st.toggle(
                    "–£—á–∏—Ç—ã–≤–∞—Ç—å robots.txt",
                    value=bool(cfg["respect_robots"]),
                    help="–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, –∞—É–¥–∏—Ç –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç URL, –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ –≤ robots.txt.",
                )
                cfg["check_external"] = st.toggle(
                    "–ü—Ä–æ–≤–µ—Ä—è—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏",
                    value=bool(cfg["check_external"]),
                    help="–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–Ω–µ—à–Ω–∏–µ URL –Ω–∞ 4xx/5xx.",
                )
                if IS_RENDER:
                    st.caption(
                        f"‚öôÔ∏è –û–±–ª–∞—á–Ω—ã–π —Ä–µ–∂–∏–º: –º–∞–∫—Å. {UI_MAX_PAGES_LIMIT} —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –¥–æ {MAX_WORKERS} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ "
                        "–¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã."
                    )

            if st.session_state["state"] == "SCANNING":
                stats = st.session_state["scan_stats"]
                st.markdown("---")
                st.write("**–ü—Ä–æ–≥—Ä–µ—Å—Å:**")
                st.progress(float(stats.get("progress", 0.0)))
                st.caption(f"–ù–∞–π–¥–µ–Ω–æ: {stats.get('found', 0)} —Å—Ç—Ä.")
                st.caption(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {stats.get('done', 0)}")
                st.caption(f"–í –æ—á–µ—Ä–µ–¥–∏: {stats.get('queue', 0)}")
                st.caption(f"–û—à–∏–±–æ–∫: {stats.get('errors', 0)}")
                st.caption(f"–í—Ä–µ–º—è: {stats.get('elapsed', 0):.1f} —Å–µ–∫")

            if st.session_state["state"] == "RESULTS" and st.session_state["results"]:
                result: SiteAuditResult = st.session_state["results"]
                report = ReportGenerator(result)
                xlsx_data = report.to_excel()
                csv_data = report.to_csv()
                domain = result.domain.replace(".", "-")
                date_str = datetime.now().strftime("%Y-%m-%d")
                st.markdown("---")
                st.success(f"‚úÖ –ê—É–¥–∏—Ç –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {result.duration} —Å–µ–∫")
                st.download_button(
                    "üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç (.xlsx)",
                    data=xlsx_data,
                    file_name=f"seo-audit-{domain}-{date_str}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True,
                )
                st.download_button(
                    "üìÑ –°–∫–∞—á–∞—Ç—å CSV",
                    data=csv_data.encode("utf-8"),
                    file_name=f"seo-audit-{domain}-{date_str}.csv",
                    mime="text/csv",
                    use_container_width=True,
                )

            if LOG_BUFFER:
                with st.expander("‚ö†Ô∏è –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"):
                    for msg in list(LOG_BUFFER)[-8:]:
                        st.caption(msg)

            st.markdown("---")
            st.caption("v1.0 ‚Ä¢ Python + Streamlit")

    def start_scan(self) -> None:
        cfg = st.session_state["config"]
        if not st.session_state.get("is_authenticated", False):
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = "‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è."
            st.rerun()

        guard = get_runtime_guard()
        session_id = st.session_state["session_id"]
        if guard.is_busy(session_id):
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = ERROR_MESSAGES_RU["busy"]
            st.rerun()

        raw_url = cfg["url"].strip()
        if not raw_url:
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = ERROR_MESSAGES_RU["invalid_url"]
            st.rerun()

        if not raw_url.startswith(("http://", "https://")):
            raw_url = "https://" + raw_url
            cfg["url"] = raw_url

        parsed = urlparse(raw_url)
        if not parsed.netloc:
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = ERROR_MESSAGES_RU["invalid_url"]
            st.rerun()

        cfg["max_pages"] = max(10, min(int(cfg.get("max_pages", UI_DEFAULT_MAX_PAGES)), UI_MAX_PAGES_LIMIT))

        st.session_state["audit_id"] = str(uuid.uuid4())[:8]
        st.session_state["scan_log"] = []
        st.session_state["scan_stats"] = {"progress": 0.0, "found": 0, "done": 0, "queue": 0, "errors": 0, "elapsed": 0.0}
        st.session_state["results"] = None
        st.session_state["error_message"] = ""
        st.session_state["state"] = "SCANNING"
        st.rerun()

    def render_welcome_screen(self) -> None:
        st.markdown(
            """
            <div style="text-align:center; padding: 20px 0 16px;">
                <h1 style="margin-bottom: 4px;">üîç SEO –ê—É–¥–∏—Ç-–ú–∞—à–∏–Ω–∞</h1>
                <p style="color:#94A3B8; font-size:16px;">–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞—É–¥–∏—Ç –ª—é–±–æ–≥–æ —Å–∞–π—Ç–∞</p>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.markdown(
            """
            <div style="background:#1E293B; border:1px solid #334155; border-radius:12px; padding:14px; text-align:center; margin-bottom:18px;">
                üåê –í–≤–µ–¥–∏—Ç–µ URL —Å–∞–π—Ç–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ ¬´–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞—É–¥–∏—Ç¬ª –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ —Å–ª–µ–≤–∞
            </div>
            """,
            unsafe_allow_html=True,
        )

        card_data = [
            ("üîß", "–¢–µ—Ö–Ω–∏–∫–∞", "–ö–æ–¥—ã –æ—Ç–≤–µ—Ç–∞ ‚Ä¢ Canonical ‚Ä¢ SSL"),
            ("üìù", "–ö–æ–Ω—Ç–µ–Ω—Ç", "Title ‚Ä¢ Description ‚Ä¢ H1"),
            ("üñºÔ∏è", "–ö–∞—Ä—Ç–∏–Ω–∫–∏", "ALT ‚Ä¢ –†–∞–∑–º–µ—Ä ‚Ä¢ CLS"),
            ("üîó", "–°—Å—ã–ª–∫–∏", "–ë–∏—Ç—ã–µ ‚Ä¢ Nofollow ‚Ä¢ –ì–ª—É–±–∏–Ω–∞"),
            ("üìä", "–†–∞–∑–º–µ—Ç–∫–∞", "Schema.org ‚Ä¢ OG ‚Ä¢ Twitter"),
            ("üîí", "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "HTTPS ‚Ä¢ HSTS ‚Ä¢ Robots"),
        ]
        cols = st.columns(3)
        for idx, (icon, title, sub) in enumerate(card_data):
            with cols[idx % 3]:
                st.markdown(
                    f"""
                    <div class="metric-card" style="min-height:128px; margin-bottom:10px;">
                        <div style="font-size:28px;">{icon}</div>
                        <div style="font-weight:700; color:#F8FAFC;">{title}</div>
                        <div style="font-size:12px; color:#94A3B8;">{sub}</div>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )

    def render_login_screen(self) -> None:
        st.markdown(
            """
            <div style="text-align:center; padding: 24px 0 12px;">
                <h1 style="margin-bottom: 6px;">üîê –î–æ—Å—Ç—É–ø –∫ SEO –ê—É–¥–∏—Ç-–ú–∞—à–∏–Ω–µ</h1>
                <p style="color:#94A3B8; font-size:16px;">
                    –í–≤–µ–¥–∏—Ç–µ –ª–æ–≥–∏–Ω –∏ –ø–∞—Ä–æ–ª—å –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ —Å–ª–µ–≤–∞.
                </p>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.markdown(
            """
            <div style="background:#1E293B; border:1px solid #334155; border-radius:12px; padding:14px; text-align:center; margin-bottom:18px;">
                –ï—Å–ª–∏ –ø—Ä–æ—Ñ–∏–ª—å –∑–∞–Ω—è—Ç ‚Äî –ø–æ–¥–æ–∂–¥–∏—Ç–µ, –ø–æ–∫–∞ —Ç–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã–π–¥–µ—Ç –∏–ª–∏ —Å–µ—Å—Å–∏—è –∏—Å—Ç–µ—á—ë—Ç –ø–æ —Ç–∞–π–º–∞—É—Ç—É.
            </div>
            """,
            unsafe_allow_html=True,
        )

    def render_scanning_screen(self) -> None:
        cfg = st.session_state["config"]
        st.subheader(f"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {urlparse(cfg['url']).netloc}")
        progress_bar = st.progress(0.0)
        stats_container = st.empty()
        log_container = st.empty()
        started = time.time()

        def callback(event: Dict[str, Any]) -> None:
            if event.get("type") == "pre_check":
                st.session_state["scan_log"].append(f"‚ÑπÔ∏è {event.get('message', '')}")
            elif event.get("type") == "page_done":
                done = int(event.get("pagesScanned", 0))
                found = int(event.get("totalDiscovered", 0))
                queue = int(event.get("queueSize", 0))
                errors = int(event.get("errorsCount", 0))
                progress = min(done / max(1, cfg["max_pages"]), 1.0)
                st.session_state["scan_stats"] = {
                    "progress": progress,
                    "found": found,
                    "done": done,
                    "queue": queue,
                    "errors": errors,
                    "elapsed": time.time() - started,
                }
                status = event.get("statusCode", 0)
                icon = "‚úÖ" if 200 <= status < 400 else "üî¥"
                st.session_state["scan_log"].append(
                    f"{icon} {event.get('url', '')}   {status}   {event.get('ttfb', 0):.2f}—Å"
                )
            elif event.get("type") == "page_error":
                st.session_state["scan_log"].append(
                    f"üî¥ {event.get('url', '')}   –æ—à–∏–±–∫–∞: {event.get('error', '')}"
                )

            st.session_state["scan_log"] = st.session_state["scan_log"][-20:]
            stats = st.session_state["scan_stats"]
            progress_bar.progress(float(stats.get("progress", 0.0)))
            stats_container.markdown(
                f"""
                <div style="display:grid; grid-template-columns: repeat(4, 1fr); gap: 10px; margin: 8px 0 12px;">
                    <div class="metric-card"><div style="font-size:24px;">{stats.get('found',0)}</div><div style="color:#94A3B8;font-size:12px;">–ù–∞–π–¥–µ–Ω–æ</div></div>
                    <div class="metric-card"><div style="font-size:24px;">{stats.get('done',0)}</div><div style="color:#94A3B8;font-size:12px;">–ì–æ—Ç–æ–≤–æ</div></div>
                    <div class="metric-card"><div style="font-size:24px;">{stats.get('queue',0)}</div><div style="color:#94A3B8;font-size:12px;">–í –æ—á–µ—Ä–µ–¥–∏</div></div>
                    <div class="metric-card"><div style="font-size:24px;">{stats.get('errors',0)}</div><div style="color:#94A3B8;font-size:12px;">–û—à–∏–±–∫–∏</div></div>
                </div>
                """,
                unsafe_allow_html=True,
            )
            log_html = "<div style='border:1px solid #334155; border-radius:12px; padding:8px; background:#0F172A;'>"
            for row in st.session_state["scan_log"][-12:]:
                row_class = "ok" if row.startswith("‚úÖ") else "error" if row.startswith("üî¥") else "info"
                log_html += f"<div class='scan-log-entry {row_class}'>{row}</div>"
            log_html += "</div>"
            log_container.markdown(log_html, unsafe_allow_html=True)

        guard = get_runtime_guard()
        session_id = st.session_state["session_id"]
        max_pages_cfg = int(cfg["max_pages"])

        if not guard.try_acquire(session_id, max_pages_cfg):
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = ERROR_MESSAGES_RU["busy"]
            st.rerun()

        try:
            crawler = CrawlerEngine(
                start_url=cfg["url"],
                max_pages=int(cfg["max_pages"]),
                max_depth=int(cfg["max_depth"]),
                crawl_delay=float(cfg["crawl_delay"]),
                respect_robots=bool(cfg["respect_robots"]),
                check_external=bool(cfg["check_external"]),
            )
            result = crawler.crawl(progress_callback=callback)
            if result.total_scanned == 0:
                st.session_state["state"] = "ERROR"
                st.session_state["error_message"] = ERROR_MESSAGES_RU["no_pages"]
            else:
                # Strip heavy fields to avoid OOM when Streamlit serializes session state
                for page in result.pages.values():
                    page.raw_html = ""
                    page.content_text = page.content_text[:2000] if page.content_text else ""
                st.session_state["results"] = result
                st.session_state["state"] = "RESULTS"
        except requests.exceptions.Timeout:
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = ERROR_MESSAGES_RU["timeout"]
        except requests.exceptions.SSLError:
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = ERROR_MESSAGES_RU["ssl_error"]
        except requests.exceptions.ConnectionError:
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = ERROR_MESSAGES_RU["connection_error"]
        except Exception as exc:
            logger.exception("–û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
            st.session_state["state"] = "ERROR"
            st.session_state["error_message"] = f"‚ùå –û—à–∏–±–∫–∞ –∞—É–¥–∏—Ç–∞: {str(exc)[:180]}"
        finally:
            guard.release(session_id)

        st.rerun()

    def render_results(self) -> None:
        result: SiteAuditResult = st.session_state["results"]
        audit_id = st.session_state.get("audit_id", "default")
        pages_payload = {url: page.to_dict() for url, page in result.pages.items()}
        pages_df = build_pages_dataframe(audit_id, pages_payload)
        issues_df = build_issues_dataframe(audit_id, pages_payload)

        tab_overview, tab_errors, tab_all_pages, tab_duplicates, tab_plan, tab_tech = st.tabs(
            ["üìä –û–±–∑–æ—Ä", "üî¥ –û—à–∏–±–∫–∏", "üìÑ –í—Å–µ —Å—Ç—Ä.", "üìù –î—É–±–ª–∏", "üéØ –ü–ª–∞–Ω", "üîß –¢–µ—Ö."]
        )
        with tab_overview:
            self.render_tab_overview(result, pages_df, issues_df)
        with tab_errors:
            self.render_tab_errors(issues_df)
        with tab_all_pages:
            self.render_tab_all_pages(result, pages_df)
        with tab_duplicates:
            self.render_tab_duplicates(result)
        with tab_plan:
            self.render_tab_action_plan(result)
        with tab_tech:
            self.render_tab_technical(result)

    def render_tab_overview(self, result: SiteAuditResult, pages_df: pd.DataFrame, issues_df: pd.DataFrame) -> None:
        score = result.health_score
        score_color = COLORS["success"] if score >= 80 else COLORS["warning"] if score >= 60 else "#F97316" if score >= 40 else COLORS["critical"]
        score_text = (
            "–û—Ç–ª–∏—á–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ" if score >= 80 else
            "–¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è" if score >= 60 else
            "–ï—Å—Ç—å —Å–µ—Ä—å—ë–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã" if score >= 40 else
            "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"
        )
        critical_issues = int(result.issues_by_severity.get("critical", 0))
        avg_ttfb = float(pages_df["TTFB (—Å–µ–∫)"].mean()) if not pages_df.empty else 0.0

        c1, c2, c3, c4 = st.columns(4)
        with c1:
            self.render_metric_card(ICONS["score"], f"{score}", "–ó–¥–æ—Ä–æ–≤—å–µ /100", score_color, score_text)
        with c2:
            self.render_metric_card(ICONS["pages"], f"{result.total_scanned}", "–°—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ", COLORS["info"])
        with c3:
            self.render_metric_card(ICONS["critical"], f"{critical_issues}", "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫", COLORS["critical"])
        with c4:
            self.render_metric_card(ICONS["speed"], f"{avg_ttfb:.2f}—Å", "–°—Ä–µ–¥–Ω–∏–π TTFB", COLORS["warning"] if avg_ttfb > 1.5 else COLORS["success"])

        left, right = st.columns(2)
        with left:
            radar_values = [
                result.category_scores.get("technical", 0),
                result.category_scores.get("content", 0),
                result.category_scores.get("images", 0),
                result.category_scores.get("links", 0),
                result.category_scores.get("structured_data", 0),
            ]
            radar = go.Figure(go.Scatterpolar(
                r=radar_values,
                theta=["–¢–µ—Ö–Ω–∏–∫–∞", "–ö–æ–Ω—Ç–µ–Ω—Ç", "–ö–∞—Ä—Ç–∏–Ω–∫–∏", "–°—Å—ã–ª–∫–∏", "–†–∞–∑–º–µ—Ç–∫–∞"],
                fill="toself",
                fillcolor="rgba(99,102,241,0.2)",
                line=dict(color=COLORS["primary"], width=2),
                marker=dict(size=8, color=COLORS["primary"]),
            ))
            radar.update_layout(
                polar=dict(
                    bgcolor="rgba(0,0,0,0)",
                    radialaxis=dict(visible=True, range=[0, 100], gridcolor="#334155", tickfont=dict(color="#64748B")),
                    angularaxis=dict(gridcolor="#334155", tickfont=dict(color="#E2E8F0")),
                )
            )
            apply_chart_style(radar, "–ó–¥–æ—Ä–æ–≤—å–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
            st.plotly_chart(radar, use_container_width=True)

        with right:
            grouped = {"200 –û–ö": 0, "3xx –†–µ–¥–∏—Ä–µ–∫—Ç": 0, "4xx –ù–µ –Ω–∞–π–¥–µ–Ω": 0, "5xx –°–µ—Ä–≤–µ—Ä": 0}
            for code_str, count in result.status_codes.items():
                try:
                    code = int(code_str)
                except ValueError:
                    continue
                if 200 <= code < 300:
                    grouped["200 –û–ö"] += count
                elif 300 <= code < 400:
                    grouped["3xx –†–µ–¥–∏—Ä–µ–∫—Ç"] += count
                elif 400 <= code < 500:
                    grouped["4xx –ù–µ –Ω–∞–π–¥–µ–Ω"] += count
                elif 500 <= code < 600:
                    grouped["5xx –°–µ—Ä–≤–µ—Ä"] += count
            donut = go.Figure(go.Pie(
                labels=list(grouped.keys()),
                values=list(grouped.values()),
                hole=0.65,
                marker=dict(colors=[COLORS["success"], COLORS["info"], COLORS["warning"], COLORS["critical"]]),
                textinfo="percent+label",
                hovertemplate="<b>%{label}</b><br>–°—Ç—Ä–∞–Ω–∏—Ü: %{value}<br>–î–æ–ª—è: %{percent}<extra></extra>",
            ))
            donut.add_annotation(
                text=f"<b>{result.total_scanned}</b><br><span style='font-size:12px;color:#94A3B8'>—Å—Ç—Ä–∞–Ω–∏—Ü</span>",
                showarrow=False,
                font=dict(size=28, color="#F8FAFC"),
            )
            apply_chart_style(donut, "–ö–æ–¥—ã –æ—Ç–≤–µ—Ç–æ–≤")
            st.plotly_chart(donut, use_container_width=True)

        slow_df = pages_df.sort_values("TTFB (—Å–µ–∫)", ascending=False).head(10)
        if not slow_df.empty:
            slow_fig = px.bar(
                slow_df,
                x="TTFB (—Å–µ–∫)",
                y="URL",
                orientation="h",
                color="TTFB (—Å–µ–∫)",
                color_continuous_scale=[COLORS["success"], COLORS["warning"], COLORS["critical"]],
            )
            slow_fig.update_layout(yaxis=dict(autorange="reversed"))
            apply_chart_style(slow_fig, "–¢–û–ü-10 —Å–∞–º—ã—Ö –º–µ–¥–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
            st.plotly_chart(slow_fig, use_container_width=True)

        if not issues_df.empty:
            err = issues_df.groupby(["–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å"]).size().reset_index(name="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
            stacked = px.bar(
                err,
                x="–ö–∞—Ç–µ–≥–æ—Ä–∏—è",
                y="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
                color="–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å",
                color_discrete_map={"critical": COLORS["critical"], "warning": COLORS["warning"], "info": COLORS["info"]},
            )
            apply_chart_style(stacked, "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
            st.plotly_chart(stacked, use_container_width=True)

    def render_tab_errors(self, issues_df: pd.DataFrame) -> None:
        if issues_df.empty:
            st.success("–ü—Ä–æ–±–ª–µ–º—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã.")
            return

        c1, c2, c3 = st.columns(3)
        with c1:
            category_filter = st.multiselect("–ö–∞—Ç–µ–≥–æ—Ä–∏—è", sorted(issues_df["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"].dropna().unique().tolist()))
        with c2:
            sev_filter = st.multiselect("–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å", ["critical", "warning", "info"], default=["critical", "warning"])
        with c3:
            code_filter = st.multiselect("–¢–∏–ø", sorted(issues_df["–ö–æ–¥"].dropna().unique().tolist()))

        filtered = issues_df.copy()
        if category_filter:
            filtered = filtered[filtered["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"].isin(category_filter)]
        if sev_filter:
            filtered = filtered[filtered["–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å"].isin(sev_filter)]
        if code_filter:
            filtered = filtered[filtered["–ö–æ–¥"].isin(code_filter)]

        order_map = {"critical": 0, "warning": 1, "info": 2}
        filtered["__sort"] = filtered["–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å"].map(order_map).fillna(9)
        filtered = filtered.sort_values("__sort").drop(columns=["__sort"])

        display_df = filtered[["URL", "–ü—Ä–æ–±–ª–µ–º–∞", "–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å", "–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ", "–ù–æ—Ä–º–∞"]].copy()
        display_df["–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å"] = display_df["–°–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å"].map({
            "critical": "üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è",
            "warning": "üü° –í–Ω–∏–º–∞–Ω–∏–µ",
            "info": "üîµ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
        })
        st.dataframe(
            display_df,
            use_container_width=True,
            column_config={"URL": st.column_config.LinkColumn("URL")},
            hide_index=True,
        )
        st.caption(f"–ü–æ–∫–∞–∑–∞–Ω–æ: {len(display_df)} –∏–∑ {len(issues_df)} –ø—Ä–æ–±–ª–µ–º")

        st.download_button(
            "üì• –°–∫–∞—á–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –æ—à–∏–±–æ–∫ (CSV)",
            data=filtered.to_csv(index=False).encode("utf-8"),
            file_name=f"errors-{datetime.now().strftime('%Y-%m-%d')}.csv",
            mime="text/csv",
            use_container_width=False,
        )

    def render_tab_all_pages(self, result: SiteAuditResult, pages_df: pd.DataFrame) -> None:
        if pages_df.empty:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º.")
            return

        q = st.text_input("üîç –ü–æ–∏—Å–∫ –ø–æ URL...", "")
        display_df = pages_df.copy()
        if q:
            display_df = display_df[display_df["URL"].str.contains(q, case=False, na=False)]
        if display_df.empty:
            st.info("–ü–æ —Ç–µ–∫—É—â–µ–º—É —Ñ–∏–ª—å—Ç—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            return
        st.dataframe(
            display_df,
            use_container_width=True,
            hide_index=True,
            column_config={"URL": st.column_config.LinkColumn("URL")},
        )

        selected = st.selectbox("–î–µ—Ç–∞–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:", options=display_df["URL"].tolist()[:200])
        page = result.pages.get(selected)
        if page:
            with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª–∏", expanded=False):
                st.write(f"**Title:** {page.title}")
                st.write(f"**Description:** {page.description}")
                st.write(f"**H1:** {', '.join(page.h1_list) if page.h1_list else '–Ω–µ—Ç'}")
                st.write(f"**–ó–∞–≥–æ–ª–æ–≤–∫–∏ H1-H6:** {page.headings}")
                for issue in page.issues:
                    st.markdown(f"- `{issue.severity}` {issue.message}")

    def render_tab_duplicates(self, result: SiteAuditResult) -> None:
        title_count = len(result.duplicate_titles)
        desc_count = len(result.duplicate_descriptions)
        content_count = len(result.duplicate_content)
        url_pattern_count = len(result.url_pattern_duplicates)

        with st.expander(f"üìù –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è Title ({title_count} –≥—Ä—É–ø–ø)", expanded=title_count > 0):
            if not result.duplicate_titles:
                st.caption("–î—É–±–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            for group in result.duplicate_titles:
                st.markdown(f"**Title:** {group['title']}")
                for url in group["urls"]:
                    st.write(f"- {url}")

        with st.expander(f"üìù –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è Description ({desc_count} –≥—Ä—É–ø–ø)", expanded=desc_count > 0):
            if not result.duplicate_descriptions:
                st.caption("–î—É–±–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            for group in result.duplicate_descriptions:
                st.markdown(f"**Description:** {group['description']}")
                for url in group["urls"]:
                    st.write(f"- {url}")

        with st.expander(f"üìù –î—É–±–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (>90% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ) ({content_count} –≥—Ä—É–ø–ø)", expanded=content_count > 0):
            if not result.duplicate_content:
                st.caption("–î—É–±–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            for idx, group in enumerate(result.duplicate_content, 1):
                st.markdown(f"**–ì—Ä—É–ø–ø–∞ {idx}:**")
                for url in group["urls"]:
                    st.write(f"- {url}")

        with st.expander(f"üìù –î—É–±–ª–∏ URL-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ ({url_pattern_count})", expanded=url_pattern_count > 0):
            if not result.url_pattern_duplicates:
                st.caption("–ù–µ –Ω–∞–π–¥–µ–Ω–æ.")
            for row in result.url_pattern_duplicates:
                st.write(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π URL: `{row['normalized']}`")
                for variant in row["variants"]:
                    st.write(f"- {variant}")

    def render_tab_action_plan(self, result: SiteAuditResult) -> None:
        recommendations = result.recommendations
        if not recommendations:
            st.success("–ö—Ä–∏—Ç–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.")
            return
        st.subheader(f"üéØ –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π ‚Äî {len(recommendations)} –∑–∞–¥–∞—á")

        for severity, title in [
            ("critical", "üî¥ –ò–°–ü–†–ê–í–ò–¢–¨ –ù–ï–ú–ï–î–õ–ï–ù–ù–û"),
            ("warning", "üü° –ò–°–ü–†–ê–í–ò–¢–¨ –í –ë–õ–ò–ñ–ê–ô–®–ï–ï –í–†–ï–ú–Ø"),
            ("info", "üîµ –£–õ–£–ß–®–ò–¢–¨ –ü–û–ó–ñ–ï"),
        ]:
            chunk = [r for r in recommendations if r["severity"] == severity]
            if not chunk:
                continue
            st.markdown(f"### {title} ({len(chunk)} –∑–∞–¥–∞—á)")
            for rec in chunk:
                self.render_recommendation_card(rec)

    def render_tab_technical(self, result: SiteAuditResult) -> None:
        st.markdown("### robots.txt")
        st.code(result.robots_txt_content or "robots.txt –Ω–µ –Ω–∞–π–¥–µ–Ω", language="text")
        if result.robots_rules:
            st.dataframe(pd.DataFrame(result.robots_rules), use_container_width=True, hide_index=True)

        st.markdown("### sitemap.xml")
        sitemap_rows = []
        crawled_set = set(result.pages.keys())
        entries = result.sitemap_entries or [{"url": u, "lastmod": ""} for u in sorted(result.sitemap_urls)]
        for entry in entries[:1000]:
            url = entry.get("url", "")
            sitemap_rows.append({
                "URL": url,
                "Lastmod": entry.get("lastmod", ""),
                "–ï—Å—Ç—å –≤ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏": "‚úÖ" if url in crawled_set else "‚ùå",
            })
        if sitemap_rows:
            st.dataframe(pd.DataFrame(sitemap_rows), use_container_width=True, hide_index=True)
        svs = result.sitemap_vs_crawled
        if svs:
            st.info(
                f"–í—Å–µ–≥–æ URL –≤ sitemap: {svs.get('sitemapTotal', 0)} | "
                f"–ù–∞–π–¥–µ–Ω–æ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: {svs.get('overlap', 0)} | "
                f"–¢–æ–ª—å–∫–æ –≤ sitemap: {len(svs.get('onlyInSitemap', []))} | "
                f"–¢–æ–ª—å–∫–æ –≤ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: {len(svs.get('onlyInCrawl', []))}"
            )

        st.markdown("### –ö–∞—Ä—Ç–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤")
        if result.redirect_map:
            rows = []
            for item in result.redirect_map:
                chain = item.get("chain", [])
                middle = " -> ".join(chain[1:-1]) if len(chain) > 2 else ""
                rows.append({
                    "–ò—Å—Ö–æ–¥–Ω—ã–π URL": chain[0] if chain else "",
                    "–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π": middle,
                    "–ö–æ–Ω–µ—á–Ω—ã–π URL": chain[-1] if chain else "",
                    "–•–æ–ø–æ–≤": item.get("hops", 0),
                    "–¢–∏–ø": item.get("type", 0),
                })
            st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
        else:
            st.caption("–†–µ–¥–∏—Ä–µ–∫—Ç—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã.")

        st.markdown("### SSL –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å")
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç", "OK" if result.ssl_valid else "–û—à–∏–±–∫–∞")
        c2.metric("TLS", result.tls_version or "n/a")
        c3.metric("HTTP ‚Üí HTTPS", "–î–∞" if result.http_to_https else "–ù–µ—Ç")
        c4.metric("WWW-–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å", result.www_consistency or "unknown")
        homepage_hsts = any(k.lower() == "strict-transport-security" for k in result.homepage_headers.keys())
        c5.metric("HSTS", "–î–∞" if homepage_hsts else "–ù–µ—Ç")

        st.markdown("### HTTP-–∑–∞–≥–æ–ª–æ–≤–∫–∏ –≥–ª–∞–≤–Ω–æ–π")
        st.caption(f"–ì–ª–∞–≤–Ω–∞—è: –∫–æ–¥ {result.homepage_status_code or 'n/a'} ‚Ä¢ TTFB {result.homepage_ttfb:.2f}—Å")
        headers_text = "\n".join([f"{k}: {v}" for k, v in result.homepage_headers.items()]) or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        st.code(headers_text, language="text")

        st.markdown("### –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞")
        if result.detected_frameworks:
            framework_list = ", ".join(result.detected_frameworks)
            st.warning(
                f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω JS-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫: {framework_list}\n\n"
                "–ß–∞—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –º–æ–∂–µ—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ JavaScript.\n"
                "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–µ—Ä–Ω—ã–π HTML."
            )
        else:
            st.success("–Ø–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã.")

        st.markdown("### –ü–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∞ –∏ —Å—Å—ã–ª–æ—á–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")
        il_c1, il_c2, il_c3, il_c4 = st.columns(4)
        il_c1.metric("–°—Ä. –≤—Ö. —Å—Å—ã–ª–æ–∫", f"{result.avg_inlinks:.1f}")
        il_c2.metric("–ú–∞–∫—Å. –≤—Ö. —Å—Å—ã–ª–æ–∫", str(result.max_inlinks))
        il_c3.metric("–¢—É–ø–∏–∫–æ–≤—ã—Ö —Å—Ç—Ä.", str(len(result.dead_end_pages)))
        il_c4.metric("–°–ª–∞–±–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö", str(len(result.pages_low_inlinks)))

        # Top pages by internal PageRank
        pr_pages = sorted(result.pages.values(), key=lambda p: p.internal_pagerank, reverse=True)
        if pr_pages:
            with st.expander("–¢–æ–ø —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É PageRank"):
                pr_data = []
                for p in pr_pages[:20]:
                    pr_data.append({
                        "URL": p.url,
                        "PageRank": p.internal_pagerank,
                        "–í—Ö. —Å—Å—ã–ª–æ–∫": p.inlink_count,
                        "–¢—É–ø–∏–∫": "–î–∞" if p.is_dead_end else "",
                    })
                st.dataframe(pd.DataFrame(pr_data), use_container_width=True, hide_index=True)

        if result.dead_end_pages:
            with st.expander(f"–¢—É–ø–∏–∫–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ({len(result.dead_end_pages)})"):
                for url in result.dead_end_pages[:50]:
                    st.write(f"- {url}")

        if result.pages_low_inlinks:
            with st.expander(f"–°—Ç—Ä–∞–Ω–∏—Ü—ã —Å <2 –≤—Ö–æ–¥—è—â–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ ({len(result.pages_low_inlinks)})"):
                for url in result.pages_low_inlinks[:50]:
                    inl = result.pages[url].inlink_count if url in result.pages else 0
                    st.write(f"- {url} ({inl} —Å—Å—ã–ª–æ–∫)")

        # Score explanation
        if result.score_explanation:
            with st.expander("–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª"):
                expl = result.score_explanation
                weights = expl.get("weights", {})
                cat_scores = expl.get("category_scores", {})
                cat_names_ru = {"technical": "–¢–µ—Ö–Ω–∏–∫–∞", "content": "–ö–æ–Ω—Ç–µ–Ω—Ç", "links": "–°—Å—ã–ª–∫–∏",
                               "images": "–ö–∞—Ä—Ç–∏–Ω–∫–∏", "structured_data": "–†–∞–∑–º–µ—Ç–∫–∞"}
                st.write("**–§–æ—Ä–º—É–ª–∞:** –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                rows = []
                for cat, weight in weights.items():
                    rows.append({
                        "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": cat_names_ru.get(cat, cat),
                        "–ë–∞–ª–ª": cat_scores.get(cat, 0),
                        "–í–µ—Å": f"{weight:.0%}",
                        "–í–∫–ª–∞–¥": f"{cat_scores.get(cat, 0) * weight:.1f}",
                    })
                st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
                st.write(f"**–ò—Ç–æ–≥–æ: {expl.get('health_score', 0)}/100**")

        st.markdown("### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ")
        st.write(f"- llms.txt: {'‚úÖ –Ω–∞–π–¥–µ–Ω' if result.has_llms_txt else '‚ùå –Ω–µ –Ω–∞–π–¥–µ–Ω'}")
        st.write(f"- URL, –∑–∞–∫—Ä—ã—Ç—ã–µ robots.txt: {len(result.robots_linked_blocked)}")
        st.write(f"- –°—Ç—Ä–∞–Ω–∏—Ü—ã –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫: {len(result.zero_inlink_pages)}")
        st.write(f"- Noindex —Å –≤—Ö–æ–¥—è—â–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏: {len(result.noindex_linked_pages)}")
        st.write(f"- –ö–ª–∞—Å—Ç–µ—Ä—ã thin content: {len(result.thin_content_clusters)}")
        st.write(f"- –¶–µ–ø–æ—á–∫–∏ canonical: {len(result.canonical_chains)}")
        st.write(f"- –ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã canonical/sitemap: {len(result.canonical_sitemap_conflicts)}")
        st.write(f"- –ü—Ä–æ–±–ª–µ–º—ã Schema-—Ä–∞–∑–º–µ—Ç–∫–∏: {len(result.schema_validation_issues)}")
        if result.robots_linked_blocked:
            with st.expander("URL, –∑–∞–∫—Ä—ã—Ç—ã–µ robots.txt, –Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–∫–∞—Ö"):
                for url in result.robots_linked_blocked[:200]:
                    st.write(f"- {url}")
        if result.canonical_chains:
            with st.expander(f"–¶–µ–ø–æ—á–∫–∏ canonical ({len(result.canonical_chains)})"):
                for chain in result.canonical_chains[:20]:
                    st.write(f"- {' ‚Üí '.join(chain['chain'][:5])}")
        if result.canonical_sitemap_conflicts:
            with st.expander(f"–ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã canonical/sitemap ({len(result.canonical_sitemap_conflicts)})"):
                for conflict in result.canonical_sitemap_conflicts[:20]:
                    st.write(f"- {conflict['url']} ‚Üí canonical: {conflict['canonical']}")

    def render_error_screen(self) -> None:
        st.error(st.session_state.get("error_message", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞."))
        if st.button("–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–Ω–æ–≤–∞"):
            st.session_state["state"] = "IDLE"
            st.session_state["error_message"] = ""
            st.rerun()

    def render_metric_card(self, icon: str, value: str, label: str, color: str, sublabel: str = "") -> None:
        st.markdown(
            f"""
            <div class="metric-card">
                <div style="font-size:28px; margin-bottom:4px;">{icon}</div>
                <div style="font-size:34px; font-weight:700; color:{color}; line-height:1.2;">{value}</div>
                <div style="font-size:14px; color:{COLORS['text_secondary']}; margin-top:4px;">{label}</div>
                <div style="font-size:12px; color:{COLORS['text_muted']};">{sublabel}</div>
            </div>
            """,
            unsafe_allow_html=True,
        )

    def render_recommendation_card(self, rec: Dict[str, Any]) -> None:
        sev = rec.get("severity", "info")
        border = COLORS["critical"] if sev == "critical" else COLORS["warning"] if sev == "warning" else COLORS["info"]
        effort = EFFORT_LABELS.get(rec.get("effort", "medium"), EFFORT_LABELS["medium"])
        st.markdown(
            f"""
            <div class="recommendation-card" style="border-left-color:{border};">
                <div style="display:flex; justify-content:space-between; gap:12px;">
                    <div style="font-size:16px; font-weight:700;">{rec.get('title')}</div>
                    <div>{severity_badge(sev)}</div>
                </div>
                <div style="margin-top:8px; color:#CBD5E1;"><b>–í–ª–∏—è–Ω–∏–µ:</b> {rec.get('impact')}</div>
                <div style="margin-top:8px; color:#CBD5E1;"><b>–ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å:</b> {rec.get('fix')}</div>
                <div style="margin-top:8px; color:#94A3B8;"><b>–°–ª–æ–∂–Ω–æ—Å—Ç—å:</b> {effort}</div>
            </div>
            """,
            unsafe_allow_html=True,
        )
        if rec.get("urls"):
            with st.expander(f"–ó–∞—Ç—Ä–æ–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ({len(rec['urls'])})"):
                for url in rec["urls"]:
                    st.write(f"- {url}")


# === MAIN ===

if __name__ == "__main__":
    app = SEOAuditApp()
    app.render()

# ============================================
# HOW TO RUN:
# 1. pip install streamlit requests beautifulsoup4 lxml pandas plotly fake-useragent openpyxl
# 2. streamlit run app.py
# 3. Open http://localhost:8501 in your browser
# 4. Enter a URL and click "Start Audit"
# ============================================
